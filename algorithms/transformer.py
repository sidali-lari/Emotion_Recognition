# -*- coding: utf-8 -*-
"""Transformer

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/176w3OeBIHAZaa3C4RoLmCwqKwkj7FWuE

# Libraires
"""

import pandas as pd
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from imblearn.over_sampling import SMOTE
from sklearn.utils import class_weight
from keras.layers import LSTM, Dense
from keras.models import Sequential
from keras.optimizers import Adam
from sklearn.metrics import classification_report
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import numpy as np
import random

"""# Import DataSet"""

df_gsr = pd.read_csv('GSR_FeaturesExtracted.csv')
df_ecg = pd.read_csv('ECG_FeaturesExtracted.csv')
df_eye_tracking = pd.read_csv('EyeTracking_FeaturesExtracted.csv')
df_all_data = pd.read_csv('all_data.csv')

"""# GSR 4 Class"""

pip install keras-transformer

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

df_gsr = pd.read_csv('GSR_FeaturesExtracted.csv')
df_ecg = pd.read_csv('ECG_FeaturesExtracted.csv')
df_eye_tracking = pd.read_csv('EyeTracking_FeaturesExtracted.csv')
#df_gsr_ecg = pd.read_csv('GSR+ECG.csv')
df_all_data = pd.read_csv('all_data.csv')

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical


X = df_gsr.drop('Quad_Cat', axis=1)
y = df_gsr['Quad_Cat']

# Scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Reshape for sequence processing
X_seq = X_scaled.reshape(-1, 1, X_scaled.shape[1])

# Label encoding
encoder = LabelEncoder()
encoded_y = encoder.fit_transform(y)
y_cat = to_categorical(encoded_y)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_seq, y_cat, test_size=0.1, random_state=42)
def build_transformer_seq_model(input_shape, num_classes):
    inputs = tf.keras.Input(shape=input_shape)

    # Setting a fixed key_dim
    key_dim_val = 128

    # 'Embedding' layer with adjusted dimensions
    embedding_layer = tf.keras.layers.Dense(key_dim_val, activation="relu")(inputs)

    # Multi-Head Self-Attention with adjusted key_dim
    attention_output = tf.keras.layers.MultiHeadAttention(
        key_dim=key_dim_val, num_heads=2, dropout=0.1
    )(embedding_layer, embedding_layer)

    # Residual connection
    attention_add = tf.keras.layers.Add()([attention_output, embedding_layer])
    attention_output_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_add)

    # Feed Forward Neural Network with adjusted dimensions
    ffn_output = tf.keras.layers.Dense(key_dim_val, activation="relu")(attention_output_norm)
    ffn_output = tf.keras.layers.Dense(key_dim_val, activation="relu")(ffn_output)
    ffn_add = tf.keras.layers.Add()([ffn_output, attention_output_norm])
    ffn_output_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ffn_add)

    # Classifier Head
    flat = tf.keras.layers.Flatten()(ffn_output_norm)
    outputs = tf.keras.layers.Dense(num_classes, activation="softmax")(flat)

    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model



# Instantiate and compile the model
transformer_model = build_transformer_seq_model(input_shape=(1, X_scaled.shape[1]), num_classes=y_cat.shape[1])
transformer_model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

# Train the model
transformer_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)

# Evaluate the model
y_pred = transformer_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Print classification report
print(classification_report(y_true, y_pred_classes))

from keras.utils import plot_model
plot_model(transformer_model, to_file = 'model_plot.png', show_shapes = True, show_layer_names=True)

plot_model(transformer_model, to_file = 'model_plot.png', show_shapes = True, show_layer_names=True)

"""# hyperparamter tuning"""

plot_model(model, to_file = 'model_plot.png', show_shapes = True, show_layer_names=True)

pip install keras-tuner

from keras_tuner import RandomSearch

from keras_tuner import RandomSearch

def build_transformer_seq_model(input_shape, num_classes, num_heads=2, key_dim_val=64, dropout_rate=0.1):
    inputs = keras.Input(shape=input_shape)

    # Embedding layer
    embedding_layer = layers.Dense(key_dim_val, activation="relu")(inputs)

    # Multi-Head Self-Attention
    attention_output = layers.MultiHeadAttention(
        key_dim=key_dim_val,
        num_heads=num_heads,
        dropout=dropout_rate
    )(embedding_layer, embedding_layer)

    attention_output = layers.LayerNormalization(epsilon=1e-6)(attention_output + embedding_layer)

    # Feed Forward Neural Network
    ffn_output = layers.Dense(128, activation="relu")(attention_output)
    ffn_output = layers.Dense(key_dim_val, activation="relu")(ffn_output)
    ffn_output = layers.LayerNormalization(epsilon=1e-6)(ffn_output + attention_output)

    # Classifier Head
    flat = layers.Flatten()(ffn_output)
    outputs = layers.Dense(num_classes, activation="softmax")(flat)

    model = keras.Model(inputs=inputs, outputs=outputs)
    return model


def build_model(hp):
    model = build_transformer_seq_model(
        input_shape=(1, X_scaled.shape[1]),
        num_classes=y_cat.shape[1],
        num_heads=hp.Int('num_heads', 2, 6, step=2),
        key_dim_val=hp.Int('key_dim_val', 64, 256, step=64),
        dropout_rate=hp.Float('dropout_rate', 0.1, 0.5, step=0.1)
    )
    model.compile(
        optimizer=tf.keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])
        ),
        loss="categorical_crossentropy",
        metrics=["accuracy"]
    )
    return model

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    directory='output',
    project_name='gsr_transformer'
)

tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=32)

plot_model(model, to_file = 'model_plot.png', show_shapes = True, show_layer_names=True)

"""# GSR Arousal"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report
from keras_tuner import RandomSearch
from tensorflow.keras.layers import Input, MultiHeadAttention, Flatten, Dense, Add, LayerNormalization
from tensorflow.keras.models import Model

# 1. Data Preprocessing for Arousal
arousal_df = df_gsr[['Quad_Cat', 'Mean', 'SD', 'Variance', 'Minimum', 'Maximum',
   'Number of Peaks', 'Number of Valleys', 'Ratio']].copy()

arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)

X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']

scaler = StandardScaler()
X_arousal_scaled = scaler.fit_transform(X_arousal)
X_arousal_reshaped = X_arousal_scaled.reshape(-1, 1, X_arousal_scaled.shape[1])

encoder = LabelEncoder()
encoder.fit(y_arousal)
encoded_y = encoder.transform(y_arousal)
y_arousal_categorical = to_categorical(encoded_y)

X_train, X_test, y_train, y_test = train_test_split(X_arousal_reshaped, y_arousal_categorical, test_size=0.1, random_state=42)
def build_transformer_seq_model(input_shape, num_classes, d_model=64, num_heads=4, feed_forward_dim=128):
    inputs = Input(shape=input_shape)

    # Projecting the input data to a denser space to match the d_model dimension
    projection = Dense(d_model)(inputs)

    # Simple transformer block
    attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(query=projection, key=projection, value=projection)
    attention_out = Flatten()(attention_out)
    x = Add()([projection, attention_out])
    x = LayerNormalization()(x)

    ffnn_out = Dense(feed_forward_dim, activation="relu")(x)
    ffnn_out = Dense(d_model)(ffnn_out)
    x = Add()([x, ffnn_out])
    x = LayerNormalization()(x)

    x = Flatten()(x)
    outputs = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs=inputs, outputs=outputs)

    return model


#attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)([inputs, inputs, inputs])


# 2 & 3. Transformer Model and Hyperparameter Tuning
def build_model(hp):
    # Hyperparameters for the transformer model
    d_model = hp.Int('d_model', min_value=32, max_value=256, step=32)
    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)
    feed_forward_dim = hp.Int('feed_forward_dim', min_value=32, max_value=256, step=32)

    model = build_transformer_seq_model(
        input_shape=(1, X_arousal_scaled.shape[1]),
        num_classes=y_arousal_categorical.shape[1],
        d_model=d_model,
        num_heads=num_heads,
        feed_forward_dim=feed_forward_dim
    )

    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='arousal_transformer_tuning',
    project_name='arousal_hyper_tuning'
)

# Run hyperparameter search
tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)

# 4. Evaluating the Model
best_model = tuner.get_best_models(num_models=1)[0]
y_pred = best_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print(classification_report(y_true, y_pred_classes))

plot_model(best_model, to_file = 'model_plot.png', show_shapes = True, show_layer_names=True)

"""# GSR 4 Class"""



import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, MultiHeadAttention, Flatten, Dense, Add, LayerNormalization
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report
from keras_tuner import RandomSearch


X = df_gsr.drop('Quad_Cat', axis=1)
y = df_gsr['Quad_Cat']

# Scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Reshape for sequence processing
X_seq = X_scaled.reshape(-1, 1, X_scaled.shape[1])

# Label encoding
encoder = LabelEncoder()
encoded_y = encoder.fit_transform(y)
y_cat = to_categorical(encoded_y)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_seq, y_cat, test_size=0.1, random_state=42)


def build_transformer_seq_model(input_shape, num_classes, d_model=64, num_heads=4, feed_forward_dim=128):
    inputs = Input(shape=input_shape)

    # Projecting the input data to a denser space to match the d_model dimension
    projection = Dense(d_model)(inputs)

    # Simple transformer block
    attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(query=projection, key=projection, value=projection)
    attention_out = Flatten()(attention_out)
    x = Add()([projection, attention_out])
    x = LayerNormalization()(x)

    ffnn_out = Dense(feed_forward_dim, activation="relu")(x)
    ffnn_out = Dense(d_model)(ffnn_out)
    x = Add()([x, ffnn_out])
    x = LayerNormalization()(x)

    x = Flatten()(x)
    outputs = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs=inputs, outputs=outputs)

    return model
# 2 & 3. Transformer Model and Hyperparameter Tuning

def build_model(hp):
    # Hyperparameters
    d_model = hp.Int('d_model', min_value=32, max_value=256, step=32)
    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)
    feed_forward_dim = hp.Int('feed_forward_dim', min_value=32, max_value=256, step=32)

    model = build_transformer_seq_model(
        input_shape=(1, X_scaled.shape[1]),
        num_classes=y_cat.shape[1],
        d_model=d_model,
        num_heads=num_heads,
        feed_forward_dim=feed_forward_dim
    )

    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='transformer_tuning',
    project_name='hyper_tuning'
)

# Hyperparameter search
tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)

# 4. Evaluating the Model
best_model = tuner.get_best_models(num_models=1)[0]
y_pred = best_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print(classification_report(y_true, y_pred_classes))

"""# GSR Valence"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, MultiHeadAttention, Flatten, Dense, Add, LayerNormalization
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report
from keras_tuner import RandomSearch


# Data Preprocessing for Valence
valence_df = df_gsr[['Quad_Cat', 'Mean', 'SD', 'Variance', 'Minimum', 'Maximum',
       'Number of Peaks', 'Number of Valleys', 'Ratio']].copy()
valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)

X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']

scaler = StandardScaler()
X_valence_scaled = scaler.fit_transform(X_valence)

X_valence_reshaped = X_valence_scaled.reshape(-1, 1, X_valence_scaled.shape[1])

encoder = LabelEncoder()
encoded_y_valence = encoder.fit_transform(y_valence)
y_valence_categorical = to_categorical(encoded_y_valence)

X_train, X_test, y_train, y_test = train_test_split(X_valence_reshaped, y_valence_categorical, test_size=0.1, random_state=42)


def build_transformer_seq_model(input_shape, num_classes, d_model=64, num_heads=4, feed_forward_dim=128):
    inputs = Input(shape=input_shape)

    # Projecting the input data to a denser space to match the d_model dimension
    projection = Dense(d_model)(inputs)

    # Simple transformer block
    attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(query=projection, key=projection, value=projection)
    attention_out = Flatten()(attention_out)
    x = Add()([projection, attention_out])
    x = LayerNormalization()(x)

    ffnn_out = Dense(feed_forward_dim, activation="relu")(x)
    ffnn_out = Dense(d_model)(ffnn_out)
    x = Add()([x, ffnn_out])
    x = LayerNormalization()(x)

    x = Flatten()(x)
    outputs = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs=inputs, outputs=outputs)

    return model
# Transformer Model and Hyperparameter Tuning
def build_model(hp):
    d_model = hp.Int('d_model', min_value=32, max_value=256, step=32)
    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)
    feed_forward_dim = hp.Int('feed_forward_dim', min_value=32, max_value=256, step=32)

    model = build_transformer_seq_model(
        input_shape=(1, X_valence_scaled.shape[1]),
        num_classes=y_valence_categorical.shape[1],
        d_model=d_model,
        num_heads=num_heads,
        feed_forward_dim=feed_forward_dim
    )

    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='valence_transformer_tuning',
    project_name='valence_hyper_tuning'
)

# Hyperparameter search
tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)

# Evaluating the Model
best_model = tuner.get_best_models(num_models=1)[0]
y_pred = best_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print(classification_report(y_true, y_pred_classes))

"""# ECG All data"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, MultiHeadAttention, Flatten, Dense, Add, LayerNormalization
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report
from keras_tuner import RandomSearch
import shutil
shutil.rmtree('transformer_tuning')


X = df_ecg.drop('Quad_Cat', axis=1)
y = df_ecg['Quad_Cat']

# Scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Reshape for sequence processing
X_seq = X_scaled.reshape(-1, 1, X_scaled.shape[1])

# Label encoding
encoder = LabelEncoder()
encoded_y = encoder.fit_transform(y)
y_cat = to_categorical(encoded_y)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_seq, y_cat, test_size=0.1, random_state=42)


def build_transformer_seq_model(input_shape, num_classes, d_model=64, num_heads=4, feed_forward_dim=128):
    inputs = Input(shape=input_shape)

    # Projecting the input data to a denser space to match the d_model dimension
    projection = Dense(d_model)(inputs)

    # Simple transformer block
    attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(query=projection, key=projection, value=projection)
    attention_out = Flatten()(attention_out)
    x = Add()([projection, attention_out])
    x = LayerNormalization()(x)

    ffnn_out = Dense(feed_forward_dim, activation="relu")(x)
    ffnn_out = Dense(d_model)(ffnn_out)
    x = Add()([x, ffnn_out])
    x = LayerNormalization()(x)

    x = Flatten()(x)
    outputs = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs=inputs, outputs=outputs)

    return model
# 2 & 3. Transformer Model and Hyperparameter Tuning

def build_model(hp):
    # Hyperparameters
    d_model = hp.Int('d_model', min_value=32, max_value=256, step=32)
    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)
    feed_forward_dim = hp.Int('feed_forward_dim', min_value=32, max_value=256, step=32)

    model = build_transformer_seq_model(
        input_shape=(1, X_scaled.shape[1]),
        num_classes=y_cat.shape[1],
        d_model=d_model,
        num_heads=num_heads,
        feed_forward_dim=feed_forward_dim
    )

    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='transformer_tuning',
    project_name='hyper_tuning'
)

# Hyperparameter search
tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)

# 4. Evaluating the Model
best_model = tuner.get_best_models(num_models=1)[0]

y_pred = best_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print(classification_report(y_true, y_pred_classes))

"""# ECG Arousal"""

from tensorflow.keras import backend as K
K.clear_session()
if os.path.exists('arousal_transformer_tuning'):
    shutil.rmtree('arousal_transformer_tuning')


# 1. Data Preprocessing for Arousal
arousal_df = df_ecg[['Quad_Cat', 'Mean', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR',
       'MaxRR', 'LF', 'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd',
       'Pnn50', 'pnn20', 'Pnn50pnn20']].copy()

arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)

X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']

scaler = StandardScaler()
X_arousal_scaled = scaler.fit_transform(X_arousal)
X_arousal_reshaped = X_arousal_scaled.reshape(-1, 1, X_arousal_scaled.shape[1])

encoder = LabelEncoder()
encoder.fit(y_arousal)
encoded_y = encoder.transform(y_arousal)
y_arousal_categorical = to_categorical(encoded_y)

X_train, X_test, y_train, y_test = train_test_split(X_arousal_reshaped, y_arousal_categorical, test_size=0.1, random_state=42)
def build_transformer_seq_model(input_shape, num_classes, d_model=64, num_heads=4, feed_forward_dim=128):
    inputs = Input(shape=input_shape)

    # Projecting the input data to a denser space to match the d_model dimension
    projection = Dense(d_model)(inputs)

    # Simple transformer block
    attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(query=projection, key=projection, value=projection)
    attention_out = Flatten()(attention_out)
    x = Add()([projection, attention_out])
    x = LayerNormalization()(x)

    ffnn_out = Dense(feed_forward_dim, activation="relu")(x)
    ffnn_out = Dense(d_model)(ffnn_out)
    x = Add()([x, ffnn_out])
    x = LayerNormalization()(x)

    x = Flatten()(x)
    outputs = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs=inputs, outputs=outputs)

    return model


#attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)([inputs, inputs, inputs])


# 2 & 3. Transformer Model and Hyperparameter Tuning
def build_model(hp):
    # Hyperparameters for the transformer model
    d_model = hp.Int('d_model', min_value=32, max_value=256, step=32)
    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)
    feed_forward_dim = hp.Int('feed_forward_dim', min_value=32, max_value=256, step=32)

    model = build_transformer_seq_model(
        input_shape=(1, X_arousal_scaled.shape[1]),
        num_classes=y_arousal_categorical.shape[1],
        d_model=d_model,
        num_heads=num_heads,
        feed_forward_dim=feed_forward_dim
    )

    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='arousal_transformer_tuning',
    project_name='arousal_hyper_tuning'
)

# Run hyperparameter search
tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)

# 4. Evaluating the Model
best_model = tuner.get_best_models(num_models=1)[0]
y_pred = best_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print(classification_report(y_true, y_pred_classes))

"""# ECG Valence"""

from tensorflow.keras import backend as K
K.clear_session()
if os.path.exists('valence_transformer_tuning'):
    shutil.rmtree('valence_transformer_tuning')


valence_df = df_ecg[['Quad_Cat', 'Mean', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR',
       'MaxRR', 'LF', 'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd',
       'Pnn50', 'pnn20', 'Pnn50pnn20']].copy()
valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)

X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']

scaler = StandardScaler()
X_valence_scaled = scaler.fit_transform(X_valence)

X_valence_reshaped = X_valence_scaled.reshape(-1, 1, X_valence_scaled.shape[1])

encoder = LabelEncoder()
encoded_y_valence = encoder.fit_transform(y_valence)
y_valence_categorical = to_categorical(encoded_y_valence)

X_train, X_test, y_train, y_test = train_test_split(X_valence_reshaped, y_valence_categorical, test_size=0.1, random_state=42)


def build_transformer_seq_model(input_shape, num_classes, d_model=64, num_heads=4, feed_forward_dim=128):
    inputs = Input(shape=input_shape)

    # Projecting the input data to a denser space to match the d_model dimension
    projection = Dense(d_model)(inputs)

    # Simple transformer block
    attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(query=projection, key=projection, value=projection)
    attention_out = Flatten()(attention_out)
    x = Add()([projection, attention_out])
    x = LayerNormalization()(x)

    ffnn_out = Dense(feed_forward_dim, activation="relu")(x)
    ffnn_out = Dense(d_model)(ffnn_out)
    x = Add()([x, ffnn_out])
    x = LayerNormalization()(x)

    x = Flatten()(x)
    outputs = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs=inputs, outputs=outputs)

    return model
# Transformer Model and Hyperparameter Tuning
def build_model(hp):
    d_model = hp.Int('d_model', min_value=32, max_value=256, step=32)
    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)
    feed_forward_dim = hp.Int('feed_forward_dim', min_value=32, max_value=256, step=32)

    model = build_transformer_seq_model(
        input_shape=(1, X_valence_scaled.shape[1]),
        num_classes=y_valence_categorical.shape[1],
        d_model=d_model,
        num_heads=num_heads,
        feed_forward_dim=feed_forward_dim
    )

    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='valence_transformer_tuning',
    project_name='valence_hyper_tuning'
)

# Hyperparameter search
tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)

# Evaluating the Model
best_model = tuner.get_best_models(num_models=1)[0]
y_pred = best_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print(classification_report(y_true, y_pred_classes))

"""# Eye Tracking all data"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, MultiHeadAttention, Flatten, Dense, Add, LayerNormalization
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report
from keras_tuner import RandomSearch
from tensorflow.keras import backend as K
K.clear_session()
if os.path.exists('valence_transformer_tuning'):
    shutil.rmtree('valence_transformer_tuning')


X = df_eye_tracking.drop('Quad_Cat', axis=1)
y = df_eye_tracking['Quad_Cat']

# Scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Reshape for sequence processing
X_seq = X_scaled.reshape(-1, 1, X_scaled.shape[1])

# Label encoding
encoder = LabelEncoder()
encoded_y = encoder.fit_transform(y)
y_cat = to_categorical(encoded_y)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_seq, y_cat, test_size=0.1, random_state=42)

#
def build_transformer_seq_model(input_shape, num_classes, d_model=64, num_heads=4, feed_forward_dim=128):
    inputs = Input(shape=input_shape)

    # Projecting the input data to a denser space to match the d_model dimension
    projection = Dense(d_model)(inputs)

    # Simple transformer block
    attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(query=projection, key=projection, value=projection)
    attention_out = Flatten()(attention_out)
    x = Add()([projection, attention_out])
    x = LayerNormalization()(x)

    ffnn_out = Dense(feed_forward_dim, activation="relu")(x)
    ffnn_out = Dense(d_model)(ffnn_out)
    x = Add()([x, ffnn_out])
    x = LayerNormalization()(x)

    x = Flatten()(x)
    outputs = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs=inputs, outputs=outputs)

    return model
# 2 & 3. Transformer Model and Hyperparameter Tuning

def build_model(hp):
    # Hyperparameters
    d_model = hp.Int('d_model', min_value=32, max_value=256, step=32)
    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)
    feed_forward_dim = hp.Int('feed_forward_dim', min_value=32, max_value=256, step=32)

    model = build_transformer_seq_model(
        input_shape=(1, X_scaled.shape[1]),
        num_classes=y_cat.shape[1],
        d_model=d_model,
        num_heads=num_heads,
        feed_forward_dim=feed_forward_dim
    )

    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='transformer_tuning',
    project_name='hyper_tuning'
)

# Hyperparameter search
tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)

# 4. Evaluating the Model
best_model = tuner.get_best_models(num_models=1)[0]

y_pred = best_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print(classification_report(y_true, y_pred_classes))

"""# Eye Tracking arousal"""

from tensorflow.keras import backend as K
K.clear_session()
if os.path.exists('arousal_transformer_tuning'):
    shutil.rmtree('arousal_transformer_tuning')


# 1. Data Preprocessing for Arousal
arousal_df = df_eye_tracking[['Quad_Cat', 'Num_of_Fixations', 'Mean_Fixation_Duration',
       'SD_Fixation_Duration', 'Skew_Fixation_Duration',
       'Max_Fixation_Duration', 'First_Fixation_Duration', 'Num_of_Saccade',
       'Mean_Saccade_Duration', 'SD_Saccade_Duration', 'Skew_Saccade_Duration',
       'Max_Saccade_Duration', 'Mean_Saccade_Amplitude',
       'SD_Saccade_Amplitude', 'Skew_Saccade_Amplitude',
       'Max_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'SD_Saccade_Direction', 'Skew_Saccade_Direction',
       'Max_Saccade_Direction', 'Mean_Saccade_Length', 'SD_Saccade_Length',
       'Skew_Saccade_Length', 'Max_Saccade_Length', 'Num_of_Blink',
       'Mean_Blink_Duration', 'SD_Blink_Duration', 'Skew_Blink_Duration',
       'Max_Blink_Duration', 'Num_of_Microsac', 'Mean_Microsac_Peak_Vel',
       'SD_Microsac_Peak_Vel', 'Skew_Microsac_Peak_Vel',
       'Max_Microsac_Peak_Vel', 'Mean_Microsac_Ampl', 'SD_Microsac_Ampl',
       'Skew_Microsac_Ampl', 'Max_Microsac_Ampl', 'Mean_Microsac_Dir',
       'SD_Microsac_Dir', 'Skew_Microsac_Dir', 'Max_Microsac_Dir',
       'Mean_Microsac_H_Amp', 'SD_Microsac_H_Amp', 'Skew_Microsac_H_Amp',
       'Max_Microsac_H_Amp', 'Mean_Microsac_V_Amp', 'SD_Microsac_V_Amp',
       'Skew_Microsac_V_Amp', 'Max_Microsac_V_Amp']].copy()

arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)

X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']

scaler = StandardScaler()
X_arousal_scaled = scaler.fit_transform(X_arousal)
X_arousal_reshaped = X_arousal_scaled.reshape(-1, 1, X_arousal_scaled.shape[1])

encoder = LabelEncoder()
encoder.fit(y_arousal)
encoded_y = encoder.transform(y_arousal)
y_arousal_categorical = to_categorical(encoded_y)

X_train, X_test, y_train, y_test = train_test_split(X_arousal_reshaped, y_arousal_categorical, test_size=0.1, random_state=42)
def build_transformer_seq_model(input_shape, num_classes, d_model=64, num_heads=4, feed_forward_dim=128):
    inputs = Input(shape=input_shape)

    # Projecting the input data to a denser space to match the d_model dimension
    projection = Dense(d_model)(inputs)

    # Simple transformer block
    attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(query=projection, key=projection, value=projection)
    attention_out = Flatten()(attention_out)
    x = Add()([projection, attention_out])
    x = LayerNormalization()(x)

    ffnn_out = Dense(feed_forward_dim, activation="relu")(x)
    ffnn_out = Dense(d_model)(ffnn_out)
    x = Add()([x, ffnn_out])
    x = LayerNormalization()(x)

    x = Flatten()(x)
    outputs = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs=inputs, outputs=outputs)

    return model


#attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)([inputs, inputs, inputs])


# 2 & 3. Transformer Model and Hyperparameter Tuning
def build_model(hp):
    # Hyperparameters for the transformer model
    d_model = hp.Int('d_model', min_value=32, max_value=256, step=32)
    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)
    feed_forward_dim = hp.Int('feed_forward_dim', min_value=32, max_value=256, step=32)

    model = build_transformer_seq_model(
        input_shape=(1, X_arousal_scaled.shape[1]),
        num_classes=y_arousal_categorical.shape[1],
        d_model=d_model,
        num_heads=num_heads,
        feed_forward_dim=feed_forward_dim
    )

    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='arousal_transformer_tuning',
    project_name='arousal_hyper_tuning'
)

# Run hyperparameter search
tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)

# 4. Evaluating the Model
best_model = tuner.get_best_models(num_models=1)[0]
y_pred = best_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print(classification_report(y_true, y_pred_classes))

"""# Eye Tracking valence"""

from tensorflow.keras import backend as K
K.clear_session()
if os.path.exists('arousal_transformer_tuning'):
    shutil.rmtree('arousal_transformer_tuning')


# 1. Data Preprocessing for Arousal
valence_df = df_eye_tracking[['Quad_Cat', 'Num_of_Fixations', 'Mean_Fixation_Duration',
       'SD_Fixation_Duration', 'Skew_Fixation_Duration',
       'Max_Fixation_Duration', 'First_Fixation_Duration', 'Num_of_Saccade',
       'Mean_Saccade_Duration', 'SD_Saccade_Duration', 'Skew_Saccade_Duration',
       'Max_Saccade_Duration', 'Mean_Saccade_Amplitude',
       'SD_Saccade_Amplitude', 'Skew_Saccade_Amplitude',
       'Max_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'SD_Saccade_Direction', 'Skew_Saccade_Direction',
       'Max_Saccade_Direction', 'Mean_Saccade_Length', 'SD_Saccade_Length',
       'Skew_Saccade_Length', 'Max_Saccade_Length', 'Num_of_Blink',
       'Mean_Blink_Duration', 'SD_Blink_Duration', 'Skew_Blink_Duration',
       'Max_Blink_Duration', 'Num_of_Microsac', 'Mean_Microsac_Peak_Vel',
       'SD_Microsac_Peak_Vel', 'Skew_Microsac_Peak_Vel',
       'Max_Microsac_Peak_Vel', 'Mean_Microsac_Ampl', 'SD_Microsac_Ampl',
       'Skew_Microsac_Ampl', 'Max_Microsac_Ampl', 'Mean_Microsac_Dir',
       'SD_Microsac_Dir', 'Skew_Microsac_Dir', 'Max_Microsac_Dir',
       'Mean_Microsac_H_Amp', 'SD_Microsac_H_Amp', 'Skew_Microsac_H_Amp',
       'Max_Microsac_H_Amp', 'Mean_Microsac_V_Amp', 'SD_Microsac_V_Amp',
       'Skew_Microsac_V_Amp', 'Max_Microsac_V_Amp']].copy()

valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)

X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']

scaler = StandardScaler()
X_valence_scaled = scaler.fit_transform(X_valence)

X_valence_reshaped = X_valence_scaled.reshape(-1, 1, X_valence_scaled.shape[1])

encoder = LabelEncoder()
encoded_y_valence = encoder.fit_transform(y_valence)
y_valence_categorical = to_categorical(encoded_y_valence)

X_train, X_test, y_train, y_test = train_test_split(X_valence_reshaped, y_valence_categorical, test_size=0.1, random_state=42)
def build_transformer_seq_model(input_shape, num_classes, d_model=64, num_heads=4, feed_forward_dim=128):
    inputs = Input(shape=input_shape)

    # Projecting the input data to a denser space to match the d_model dimension
    projection = Dense(d_model)(inputs)

    # Simple transformer block
    attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(query=projection, key=projection, value=projection)
    attention_out = Flatten()(attention_out)
    x = Add()([projection, attention_out])
    x = LayerNormalization()(x)

    ffnn_out = Dense(feed_forward_dim, activation="relu")(x)
    ffnn_out = Dense(d_model)(ffnn_out)
    x = Add()([x, ffnn_out])
    x = LayerNormalization()(x)

    x = Flatten()(x)
    outputs = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs=inputs, outputs=outputs)

    return model


#attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)([inputs, inputs, inputs])


# 2 & 3. Transformer Model and Hyperparameter Tuning
def build_model(hp):
    # Hyperparameters for the transformer model
    d_model = hp.Int('d_model', min_value=32, max_value=256, step=32)
    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)
    feed_forward_dim = hp.Int('feed_forward_dim', min_value=32, max_value=256, step=32)

    model = build_transformer_seq_model(
        input_shape=(1, X_arousal_scaled.shape[1]),
        num_classes=y_arousal_categorical.shape[1],
        d_model=d_model,
        num_heads=num_heads,
        feed_forward_dim=feed_forward_dim
    )

    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='arousal_transformer_tuning',
    project_name='arousal_hyper_tuning'
)

# Run hyperparameter search
tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)

# 4. Evaluating the Model
best_model = tuner.get_best_models(num_models=1)[0]
y_pred = best_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print(classification_report(y_true, y_pred_classes))

"""# all data - 4 class"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, MultiHeadAttention, Flatten, Dense, Add, LayerNormalization
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report
from keras_tuner import RandomSearch
from tensorflow.keras import backend as K
K.clear_session()
if os.path.exists('arousal_transformer_tuning'):
    shutil.rmtree('arousal_transformer_tuning')
import shutil
if os.path.exists('transformer_tuning'):
    shutil.rmtree('transformer_tuning')


X = df_all_data.drop('Quad_Cat', axis=1)
y = df_all_data['Quad_Cat']

# Scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Reshape for sequence processing
X_seq = X_scaled.reshape(-1, 1, X_scaled.shape[1])

# Label encoding
encoder = LabelEncoder()
encoded_y = encoder.fit_transform(y)
y_cat = to_categorical(encoded_y)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_seq, y_cat, test_size=0.1, random_state=42)


def build_transformer_seq_model(input_shape, num_classes, d_model=64, num_heads=4, feed_forward_dim=128):
    inputs = Input(shape=input_shape)

    # Projecting the input data to a denser space to match the d_model dimension
    projection = Dense(d_model)(inputs)

    # Simple transformer block
    attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(query=projection, key=projection, value=projection)
    attention_out = Flatten()(attention_out)
    x = Add()([projection, attention_out])
    x = LayerNormalization()(x)

    ffnn_out = Dense(feed_forward_dim, activation="relu")(x)
    ffnn_out = Dense(d_model)(ffnn_out)
    x = Add()([x, ffnn_out])
    x = LayerNormalization()(x)

    x = Flatten()(x)
    outputs = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs=inputs, outputs=outputs)

    return model
# 2 & 3. Transformer Model and Hyperparameter Tuning

def build_model(hp):
    # Hyperparameters
    d_model = hp.Int('d_model', min_value=32, max_value=256, step=32)
    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)
    feed_forward_dim = hp.Int('feed_forward_dim', min_value=32, max_value=256, step=32)

    model = build_transformer_seq_model(
        input_shape=(1, X_scaled.shape[1]),
        num_classes=y_cat.shape[1],
        d_model=d_model,
        num_heads=num_heads,
        feed_forward_dim=feed_forward_dim
    )

    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='transformer_tuning',
    project_name='hyper_tuning'
)

## Hyperparameter search
tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)

# 4. Evaluating the Model
best_model = tuner.get_best_models(num_models=1)[0]

y_pred = best_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print(classification_report(y_true, y_pred_classes))

"""# all data arousal"""

from tensorflow.keras import backend as K
K.clear_session()
if os.path.exists('arousal_transformer_tuning'):
    shutil.rmtree('arousal_transformer_tuning')


# 1. Data Preprocessing for Arousal
arousal_df = df_all_data[['Quad_Cat', 'Num_of_Fixations', 'Mean_Fixation_Duration',
       'SD_Fixation_Duration', 'Skew_Fixation_Duration',
       'Max_Fixation_Duration', 'First_Fixation_Duration', 'Num_of_Saccade',
       'Mean_Saccade_Duration', 'SD_Saccade_Duration', 'Skew_Saccade_Duration',
       'Max_Saccade_Duration', 'Mean_Saccade_Amplitude',
       'SD_Saccade_Amplitude', 'Skew_Saccade_Amplitude',
       'Max_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'SD_Saccade_Direction', 'Skew_Saccade_Direction',
       'Max_Saccade_Direction', 'Mean_Saccade_Length', 'SD_Saccade_Length',
       'Skew_Saccade_Length', 'Max_Saccade_Length', 'Num_of_Blink',
       'Mean_Blink_Duration', 'SD_Blink_Duration', 'Skew_Blink_Duration',
       'Max_Blink_Duration', 'Num_of_Microsac', 'Mean_Microsac_Peak_Vel',
       'SD_Microsac_Peak_Vel', 'Skew_Microsac_Peak_Vel',
       'Max_Microsac_Peak_Vel', 'Mean_Microsac_Ampl', 'SD_Microsac_Ampl',
       'Skew_Microsac_Ampl', 'Max_Microsac_Ampl', 'Mean_Microsac_Dir',
       'SD_Microsac_Dir', 'Skew_Microsac_Dir', 'Max_Microsac_Dir',
       'Mean_Microsac_H_Amp', 'SD_Microsac_H_Amp', 'Skew_Microsac_H_Amp',
       'Max_Microsac_H_Amp', 'Mean_Microsac_V_Amp', 'SD_Microsac_V_Amp',
       'Skew_Microsac_V_Amp', 'Max_Microsac_V_Amp', 'Mean', 'SD', 'Variance',
       'Minimum', 'Maximum', 'Number of Peaks', 'Number of Valleys', 'Ratio',
       'Mean.1', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR', 'MaxRR', 'LF',
       'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd', 'Pnn50', 'pnn20',
       'Pnn50pnn20']].copy()

arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)

X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']

scaler = StandardScaler()
X_arousal_scaled = scaler.fit_transform(X_arousal)
X_arousal_reshaped = X_arousal_scaled.reshape(-1, 1, X_arousal_scaled.shape[1])

encoder = LabelEncoder()
encoder.fit(y_arousal)
encoded_y = encoder.transform(y_arousal)
y_arousal_categorical = to_categorical(encoded_y)

X_train, X_test, y_train, y_test = train_test_split(X_arousal_reshaped, y_arousal_categorical, test_size=0.1, random_state=42)
def build_transformer_seq_model(input_shape, num_classes, d_model=64, num_heads=4, feed_forward_dim=128):
    inputs = Input(shape=input_shape)

    # Projecting the input data to a denser space to match the d_model dimension
    projection = Dense(d_model)(inputs)

    # Simple transformer block
    attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(query=projection, key=projection, value=projection)
    attention_out = Flatten()(attention_out)
    x = Add()([projection, attention_out])
    x = LayerNormalization()(x)

    ffnn_out = Dense(feed_forward_dim, activation="relu")(x)
    ffnn_out = Dense(d_model)(ffnn_out)
    x = Add()([x, ffnn_out])
    x = LayerNormalization()(x)

    x = Flatten()(x)
    outputs = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs=inputs, outputs=outputs)

    return model


#attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)([inputs, inputs, inputs])


# 2 & 3. Transformer Model and Hyperparameter Tuning
def build_model(hp):
    # Hyperparameters for the transformer model
    d_model = hp.Int('d_model', min_value=32, max_value=256, step=32)
    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)
    feed_forward_dim = hp.Int('feed_forward_dim', min_value=32, max_value=256, step=32)

    model = build_transformer_seq_model(
        input_shape=(1, X_arousal_scaled.shape[1]),
        num_classes=y_arousal_categorical.shape[1],
        d_model=d_model,
        num_heads=num_heads,
        feed_forward_dim=feed_forward_dim
    )

    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='arousal_transformer_tuning',
    project_name='arousal_hyper_tuning'
)

# Run hyperparameter search
tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)

# 4. Evaluating the Model
best_model = tuner.get_best_models(num_models=1)[0]
y_pred = best_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print(classification_report(y_true, y_pred_classes))

"""# all data valence"""

from tensorflow.keras import backend as K
K.clear_session()
if os.path.exists('arousal_transformer_tuning'):
    shutil.rmtree('arousal_transformer_tuning')


# 1. Data Preprocessing for Arousal
valence_df = df_all_data[['Quad_Cat', 'Num_of_Fixations', 'Mean_Fixation_Duration',
       'SD_Fixation_Duration', 'Skew_Fixation_Duration',
       'Max_Fixation_Duration', 'First_Fixation_Duration', 'Num_of_Saccade',
       'Mean_Saccade_Duration', 'SD_Saccade_Duration', 'Skew_Saccade_Duration',
       'Max_Saccade_Duration', 'Mean_Saccade_Amplitude',
       'SD_Saccade_Amplitude', 'Skew_Saccade_Amplitude',
       'Max_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'SD_Saccade_Direction', 'Skew_Saccade_Direction',
       'Max_Saccade_Direction', 'Mean_Saccade_Length', 'SD_Saccade_Length',
       'Skew_Saccade_Length', 'Max_Saccade_Length', 'Num_of_Blink',
       'Mean_Blink_Duration', 'SD_Blink_Duration', 'Skew_Blink_Duration',
       'Max_Blink_Duration', 'Num_of_Microsac', 'Mean_Microsac_Peak_Vel',
       'SD_Microsac_Peak_Vel', 'Skew_Microsac_Peak_Vel',
       'Max_Microsac_Peak_Vel', 'Mean_Microsac_Ampl', 'SD_Microsac_Ampl',
       'Skew_Microsac_Ampl', 'Max_Microsac_Ampl', 'Mean_Microsac_Dir',
       'SD_Microsac_Dir', 'Skew_Microsac_Dir', 'Max_Microsac_Dir',
       'Mean_Microsac_H_Amp', 'SD_Microsac_H_Amp', 'Skew_Microsac_H_Amp',
       'Max_Microsac_H_Amp', 'Mean_Microsac_V_Amp', 'SD_Microsac_V_Amp',
       'Skew_Microsac_V_Amp', 'Max_Microsac_V_Amp', 'Mean', 'SD', 'Variance',
       'Minimum', 'Maximum', 'Number of Peaks', 'Number of Valleys', 'Ratio',
       'Mean.1', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR', 'MaxRR', 'LF',
       'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd', 'Pnn50', 'pnn20',
       'Pnn50pnn20']].copy()

valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)

X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']

scaler = StandardScaler()
X_valence_scaled = scaler.fit_transform(X_valence)

X_valence_reshaped = X_valence_scaled.reshape(-1, 1, X_valence_scaled.shape[1])

encoder = LabelEncoder()
encoded_y_valence = encoder.fit_transform(y_valence)
y_valence_categorical = to_categorical(encoded_y_valence)

X_train, X_test, y_train, y_test = train_test_split(X_valence_reshaped, y_valence_categorical, test_size=0.1, random_state=42)
def build_transformer_seq_model(input_shape, num_classes, d_model=64, num_heads=4, feed_forward_dim=128):
    inputs = Input(shape=input_shape)

    # Projecting the input data to a denser space to match the d_model dimension
    projection = Dense(d_model)(inputs)

    # Simple transformer block
    attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(query=projection, key=projection, value=projection)
    attention_out = Flatten()(attention_out)
    x = Add()([projection, attention_out])
    x = LayerNormalization()(x)

    ffnn_out = Dense(feed_forward_dim, activation="relu")(x)
    ffnn_out = Dense(d_model)(ffnn_out)
    x = Add()([x, ffnn_out])
    x = LayerNormalization()(x)

    x = Flatten()(x)
    outputs = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs=inputs, outputs=outputs)

    return model


#attention_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)([inputs, inputs, inputs])


# 2 & 3. Transformer Model and Hyperparameter Tuning
def build_model(hp):
    # Hyperparameters for the transformer model
    d_model = hp.Int('d_model', min_value=32, max_value=256, step=32)
    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)
    feed_forward_dim = hp.Int('feed_forward_dim', min_value=32, max_value=256, step=32)

    model = build_transformer_seq_model(
        input_shape=(1, X_arousal_scaled.shape[1]),
        num_classes=y_arousal_categorical.shape[1],
        d_model=d_model,
        num_heads=num_heads,
        feed_forward_dim=feed_forward_dim
    )

    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='arousal_transformer_tuning',
    project_name='arousal_hyper_tuning'
)

# Run hyperparameter search
tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)

# 4. Evaluating the Model
best_model = tuner.get_best_models(num_models=1)[0]
y_pred = best_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print(classification_report(y_true, y_pred_classes))