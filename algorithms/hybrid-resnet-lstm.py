# -*- coding: utf-8 -*-
"""Hybrid_ResNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tZ6x5vNVR1TwwrQGLPI6-Ia1y0RYuukh

# Libraries
"""

import pandas as pd
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from imblearn.over_sampling import SMOTE
from sklearn.utils import class_weight
from keras.layers import LSTM, Dense
from keras.models import Sequential
from keras.optimizers import Adam
from sklearn.metrics import classification_report
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import numpy as np
import random

"""# importing dataset"""

df_gsr = pd.read_csv('GSR_FeaturesExtracted.csv')
df_ecg = pd.read_csv('ECG_FeaturesExtracted.csv')
df_eye_tracking = pd.read_csv('EyeTracking_FeaturesExtracted.csv')
df_all_data = pd.read_csv('all_data.csv')

"""# GSR 4-CLASS"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Add, GlobalAveragePooling1D, Dense, LSTM
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.metrics import classification_report


X = df_gsr.drop('Quad_Cat', axis=1).values
y = df_gsr['Quad_Cat'].values

# Scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Converting labels to integer then to categorical
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)
y_categorical = to_categorical(y_encoded)

# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_categorical, test_size=0.1, random_state=42)

# Reshape data
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

input_shape = X_train_reshaped.shape[1:]
num_classes = y_categorical.shape[1]

# ResNet block
def resnet_block(input_data, filters, conv_size):
    x = Conv1D(filters, conv_size, activation='relu', padding='same')(input_data)
    x = BatchNormalization()(x)
    x = Conv1D(filters, conv_size, activation=None, padding='same')(x)
    x = BatchNormalization()(x)


    if input_data.shape[-1] != filters:
        input_data = Conv1D(filters, 1, activation=None, padding='same')(input_data)
        input_data = BatchNormalization()(input_data)

    x = Add()([x, input_data])
    x = Activation('relu')(x)
    return x

# Hybrid ResNet-LSTM model
def build_resnet_lstm_model(input_shape, num_classes, filters=32, lstm_units=50):
    inputs = Input(shape=input_shape)

    x = Conv1D(16, 3, activation='relu')(inputs)
    x = Conv1D(32, 3, activation='relu')(x)
    x = BatchNormalization()(x)

    x = resnet_block(x, filters, 3)
    x = resnet_block(x, filters, 3)

    x = LSTM(lstm_units, return_sequences=True)(x)
    x = LSTM(lstm_units)(x)

    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Model with hyperparameter configurations
def create_model(filters=32, lstm_units=50, learning_rate=0.001):
    model = build_resnet_lstm_model(input_shape, num_classes, filters=filters, lstm_units=lstm_units)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)

param_grid = {
    'filters': [16, 32, 64],
    'lstm_units': [30, 50, 70],
    'learning_rate': [0.001, 0.01, 0.1]
}

search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=1)
search_result = search.fit(X_train_reshaped, y_train)

print("Best: %f using %s" % (search_result.best_score_, search_result.best_params_))

# Predict on test set
best_model = search.best_estimator_.model
y_pred = best_model.predict(X_test_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Print classification report
print(classification_report(y_true, y_pred_classes))

from keras.utils import plot_model
plot_model(best_model, to_file = 'model_plot.png', show_shapes = True, show_layer_names=True)

"""# hybrid GSR - arousal"""

arousal_df = df_gsr[['Quad_Cat', 'Mean', 'SD', 'Variance', 'Minimum', 'Maximum',
   'Number of Peaks', 'Number of Valleys', 'Ratio']].copy()

arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)

X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']

scaler = StandardScaler()
X_arousal_scaled = scaler.fit_transform(X_arousal)
X_arousal_reshaped = X_arousal_scaled.reshape(-1, 1, X_arousal_scaled.shape[1])

encoder = LabelEncoder()
encoder.fit(y_arousal)
encoded_y = encoder.transform(y_arousal)
y_arousal_categorical = to_categorical(encoded_y)




# No need to reshape X_arousal_scaled here
X_train, X_test, y_train, y_test = train_test_split(X_arousal_scaled, y_arousal_categorical, test_size=0.1, random_state=42)

# Reshape data
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)



input_shape = X_train_reshaped.shape[1:]
#num_classes = y_categorical.shape[1]
num_classes = y_arousal_categorical.shape[1]

# ResNet block
def resnet_block(input_data, filters, conv_size):
    x = Conv1D(filters, conv_size, activation='relu', padding='same')(input_data)
    x = BatchNormalization()(x)
    x = Conv1D(filters, conv_size, activation=None, padding='same')(x)
    x = BatchNormalization()(x)


    if input_data.shape[-1] != filters:
        input_data = Conv1D(filters, 1, activation=None, padding='same')(input_data)
        input_data = BatchNormalization()(input_data)

    x = Add()([x, input_data])
    x = Activation('relu')(x)
    return x

# Hybrid ResNet-LSTM model
def build_resnet_lstm_model(input_shape, num_classes, filters=32, lstm_units=50):
    inputs = Input(shape=input_shape)

    x = Conv1D(16, 3, activation='relu')(inputs)
    x = Conv1D(32, 3, activation='relu')(x)
    x = BatchNormalization()(x)

    x = resnet_block(x, filters, 3)
    x = resnet_block(x, filters, 3)

    x = LSTM(lstm_units, return_sequences=True)(x)
    x = LSTM(lstm_units)(x)

    #outputs = Dense(num_classes, activation='softmax')(x)
    outputs = Dense(2, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Model with hyperparameter configurations
def create_model(filters=32, lstm_units=50, learning_rate=0.001):
    model = build_resnet_lstm_model(input_shape, num_classes, filters=filters, lstm_units=lstm_units)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)

param_grid = {
    'filters': [16, 32, 64],
    'lstm_units': [30, 50, 70],
    'learning_rate': [0.001, 0.01, 0.1]
}

search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=1)
search_result = search.fit(X_train_reshaped, y_train)

print("Best: %f using %s" % (search_result.best_score_, search_result.best_params_))

# Predict on test set
best_model = search.best_estimator_.model
y_pred = best_model.predict(X_test_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Print classification report
print(classification_report(y_true, y_pred_classes))

"""# gsr valence"""

valence_df = df_gsr[['Quad_Cat', 'Mean', 'SD', 'Variance', 'Minimum', 'Maximum',
       'Number of Peaks', 'Number of Valleys', 'Ratio']].copy()

valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)

X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']

scaler = StandardScaler()
X_valence_scaled = scaler.fit_transform(
X_valence)
X_valence_reshaped = X_valence_scaled.reshape(-1, 1, X_valence_scaled.shape[1])

encoder = LabelEncoder()
encoder.fit(y_valence)
encoded_y = encoder.transform(y_valence)
y_valence_categorical = to_categorical(encoded_y)




# No need to reshape X_valence_scaled here
X_train, X_test, y_train, y_test = train_test_split(X_valence_scaled, y_valence_categorical, test_size=0.1, random_state=42)

# Reshape data
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)



input_shape = X_train_reshaped.shape[1:]
#num_classes = y_categorical.shape[1]
num_classes = y_valence_categorical.shape[1]

# ResNet block
def resnet_block(input_data, filters, conv_size):
    x = Conv1D(filters, conv_size, activation='relu', padding='same')(input_data)
    x = BatchNormalization()(x)
    x = Conv1D(filters, conv_size, activation=None, padding='same')(x)
    x = BatchNormalization()(x)


    if input_data.shape[-1] != filters:
        input_data = Conv1D(filters, 1, activation=None, padding='same')(input_data)
        input_data = BatchNormalization()(input_data)

    x = Add()([x, input_data])
    x = Activation('relu')(x)
    return x

# Hybrid ResNet-LSTM model
def build_resnet_lstm_model(input_shape, num_classes, filters=32, lstm_units=50):
    inputs = Input(shape=input_shape)

    x = Conv1D(16, 3, activation='relu')(inputs)
    x = Conv1D(32, 3, activation='relu')(x)
    x = BatchNormalization()(x)

    x = resnet_block(x, filters, 3)
    x = resnet_block(x, filters, 3)

    x = LSTM(lstm_units, return_sequences=True)(x)
    x = LSTM(lstm_units)(x)

    #outputs = Dense(num_classes, activation='softmax')(x)
    outputs = Dense(2, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Model with hyperparameter configurations
def create_model(filters=32, lstm_units=50, learning_rate=0.001):
    model = build_resnet_lstm_model(input_shape, num_classes, filters=filters, lstm_units=lstm_units)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)

param_grid = {
    'filters': [16, 32, 64],
    'lstm_units': [30, 50, 70],
    'learning_rate': [0.001, 0.01, 0.1]
}

search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=1)
search_result = search.fit(X_train_reshaped, y_train)

print("Best: %f using %s" % (search_result.best_score_, search_result.best_params_))

# Predict on test set
best_model = search.best_estimator_.model
y_pred = best_model.predict(X_test_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Print classification report
print(classification_report(y_true, y_pred_classes))

"""# ECG 4 CLASS"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Add, GlobalAveragePooling1D, Dense, LSTM
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.metrics import classification_report


X = df_ecg.drop('Quad_Cat', axis=1).values
y = df_ecg['Quad_Cat'].values

# Scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Converting labels to integer then to categorical
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)
y_categorical = to_categorical(y_encoded)

# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_categorical, test_size=0.1, random_state=42)

# Reshape data
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

input_shape = X_train_reshaped.shape[1:]
num_classes = y_categorical.shape[1]

# ResNet block
def resnet_block(input_data, filters, conv_size):
    x = Conv1D(filters, conv_size, activation='relu', padding='same')(input_data)
    x = BatchNormalization()(x)
    x = Conv1D(filters, conv_size, activation=None, padding='same')(x)
    x = BatchNormalization()(x)


    if input_data.shape[-1] != filters:
        input_data = Conv1D(filters, 1, activation=None, padding='same')(input_data)
        input_data = BatchNormalization()(input_data)

    x = Add()([x, input_data])
    x = Activation('relu')(x)
    return x

# Hybrid ResNet-LSTM model
def build_resnet_lstm_model(input_shape, num_classes, filters=32, lstm_units=50):
    inputs = Input(shape=input_shape)

    x = Conv1D(16, 3, activation='relu')(inputs)
    x = Conv1D(32, 3, activation='relu')(x)
    x = BatchNormalization()(x)

    x = resnet_block(x, filters, 3)
    x = resnet_block(x, filters, 3)

    x = LSTM(lstm_units, return_sequences=True)(x)
    x = LSTM(lstm_units)(x)

    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Model with hyperparameter configurations
def create_model(filters=32, lstm_units=50, learning_rate=0.001):
    model = build_resnet_lstm_model(input_shape, num_classes, filters=filters, lstm_units=lstm_units)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)

param_grid = {
    'filters': [16, 32, 64],
    'lstm_units': [30, 50, 70],
    'learning_rate': [0.001, 0.01, 0.1]
}

search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=1)
search_result = search.fit(X_train_reshaped, y_train)

print("Best: %f using %s" % (search_result.best_score_, search_result.best_params_))

# Predict on test set
best_model = search.best_estimator_.model
y_pred = best_model.predict(X_test_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Print classification report
print(classification_report(y_true, y_pred_classes))

"""# ECG AROUSAL"""

arousal_df = df_ecg[['Quad_Cat', 'Mean', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR',
       'MaxRR', 'LF', 'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd',
       'Pnn50', 'pnn20', 'Pnn50pnn20']].copy()

arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)

X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']

scaler = StandardScaler()
X_arousal_scaled = scaler.fit_transform(X_arousal)
X_arousal_reshaped = X_arousal_scaled.reshape(-1, 1, X_arousal_scaled.shape[1])

encoder = LabelEncoder()
encoder.fit(y_arousal)
encoded_y = encoder.transform(y_arousal)
y_arousal_categorical = to_categorical(encoded_y)





X_train, X_test, y_train, y_test = train_test_split(X_arousal_scaled, y_arousal_categorical, test_size=0.1, random_state=42)


X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)



input_shape = X_train_reshaped.shape[1:]
#num_classes = y_categorical.shape[1]
num_classes = y_arousal_categorical.shape[1]

# ResNet block
def resnet_block(input_data, filters, conv_size):
    x = Conv1D(filters, conv_size, activation='relu', padding='same')(input_data)
    x = BatchNormalization()(x)
    x = Conv1D(filters, conv_size, activation=None, padding='same')(x)
    x = BatchNormalization()(x)


    if input_data.shape[-1] != filters:
        input_data = Conv1D(filters, 1, activation=None, padding='same')(input_data)
        input_data = BatchNormalization()(input_data)

    x = Add()([x, input_data])
    x = Activation('relu')(x)
    return x

# Hybrid ResNet-LSTM model
def build_resnet_lstm_model(input_shape, num_classes, filters=32, lstm_units=50):
    inputs = Input(shape=input_shape)

    x = Conv1D(16, 3, activation='relu')(inputs)
    x = Conv1D(32, 3, activation='relu')(x)
    x = BatchNormalization()(x)

    x = resnet_block(x, filters, 3)
    x = resnet_block(x, filters, 3)

    x = LSTM(lstm_units, return_sequences=True)(x)
    x = LSTM(lstm_units)(x)

    #outputs = Dense(num_classes, activation='softmax')(x)
    outputs = Dense(2, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Model with hyperparameter configurations
def create_model(filters=32, lstm_units=50, learning_rate=0.001):
    model = build_resnet_lstm_model(input_shape, num_classes, filters=filters, lstm_units=lstm_units)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)

param_grid = {
    'filters': [16, 32, 64],
    'lstm_units': [30, 50, 70],
    'learning_rate': [0.001, 0.01, 0.1]
}

search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=1)
search_result = search.fit(X_train_reshaped, y_train)

print("Best: %f using %s" % (search_result.best_score_, search_result.best_params_))

# Predict on test set
best_model = search.best_estimator_.model
y_pred = best_model.predict(X_test_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Print classification report
print(classification_report(y_true, y_pred_classes))

"""# ECG valence"""

valence_df = df_ecg[['Quad_Cat', 'Mean', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR',
       'MaxRR', 'LF', 'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd',
       'Pnn50', 'pnn20', 'Pnn50pnn20']].copy()

valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)

X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']

scaler = StandardScaler()
X_valence_scaled = scaler.fit_transform(
X_valence)
X_valence_reshaped = X_valence_scaled.reshape(-1, 1, X_valence_scaled.shape[1])

encoder = LabelEncoder()
encoder.fit(y_valence)
encoded_y = encoder.transform(y_valence)
y_valence_categorical = to_categorical(encoded_y)





X_train, X_test, y_train, y_test = train_test_split(X_valence_scaled, y_valence_categorical, test_size=0.1, random_state=42)


X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)



input_shape = X_train_reshaped.shape[1:]
#num_classes = y_categorical.shape[1]
num_classes = y_valence_categorical.shape[1]

# ResNet block
def resnet_block(input_data, filters, conv_size):
    x = Conv1D(filters, conv_size, activation='relu', padding='same')(input_data)
    x = BatchNormalization()(x)
    x = Conv1D(filters, conv_size, activation=None, padding='same')(x)
    x = BatchNormalization()(x)


    if input_data.shape[-1] != filters:
        input_data = Conv1D(filters, 1, activation=None, padding='same')(input_data)
        input_data = BatchNormalization()(input_data)

    x = Add()([x, input_data])
    x = Activation('relu')(x)
    return x

# Hybrid ResNet-LSTM model
def build_resnet_lstm_model(input_shape, num_classes, filters=32, lstm_units=50):
    inputs = Input(shape=input_shape)

    x = Conv1D(16, 3, activation='relu')(inputs)
    x = Conv1D(32, 3, activation='relu')(x)
    x = BatchNormalization()(x)

    x = resnet_block(x, filters, 3)
    x = resnet_block(x, filters, 3)

    x = LSTM(lstm_units, return_sequences=True)(x)
    x = LSTM(lstm_units)(x)

    #outputs = Dense(num_classes, activation='softmax')(x)
    outputs = Dense(2, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Model with hyperparameter configurations
def create_model(filters=32, lstm_units=50, learning_rate=0.001):
    model = build_resnet_lstm_model(input_shape, num_classes, filters=filters, lstm_units=lstm_units)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)

param_grid = {
    'filters': [16, 32, 64],
    'lstm_units': [30, 50, 70],
    'learning_rate': [0.001, 0.01, 0.1]
}

search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=1)
search_result = search.fit(X_train_reshaped, y_train)

print("Best: %f using %s" % (search_result.best_score_, search_result.best_params_))

# Predict on test set
best_model = search.best_estimator_.model
y_pred = best_model.predict(X_test_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Print classification report
print(classification_report(y_true, y_pred_classes))

"""# eye tracking 4 class"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Add, GlobalAveragePooling1D, Dense, LSTM
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.metrics import classification_report


X = df_eye_tracking.drop('Quad_Cat', axis=1).values
y = df_eye_tracking['Quad_Cat'].values

# Scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Converting labels to integer then to categorical
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)
y_categorical = to_categorical(y_encoded)

# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_categorical, test_size=0.1, random_state=42)

# Reshape data
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

input_shape = X_train_reshaped.shape[1:]
num_classes = y_categorical.shape[1]

# ResNet block
def resnet_block(input_data, filters, conv_size):
    x = Conv1D(filters, conv_size, activation='relu', padding='same')(input_data)
    x = BatchNormalization()(x)
    x = Conv1D(filters, conv_size, activation=None, padding='same')(x)
    x = BatchNormalization()(x)


    if input_data.shape[-1] != filters:
        input_data = Conv1D(filters, 1, activation=None, padding='same')(input_data)
        input_data = BatchNormalization()(input_data)

    x = Add()([x, input_data])
    x = Activation('relu')(x)
    return x

# Hybrid ResNet-LSTM model
def build_resnet_lstm_model(input_shape, num_classes, filters=32, lstm_units=50):
    inputs = Input(shape=input_shape)

    x = Conv1D(16, 3, activation='relu')(inputs)
    x = Conv1D(32, 3, activation='relu')(x)
    x = BatchNormalization()(x)

    x = resnet_block(x, filters, 3)
    x = resnet_block(x, filters, 3)

    x = LSTM(lstm_units, return_sequences=True)(x)
    x = LSTM(lstm_units)(x)

    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Model with hyperparameter configurations
def create_model(filters=32, lstm_units=50, learning_rate=0.001):
    model = build_resnet_lstm_model(input_shape, num_classes, filters=filters, lstm_units=lstm_units)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)

param_grid = {
    'filters': [16, 32, 64],
    'lstm_units': [30, 50, 70],
    'learning_rate': [0.001, 0.01, 0.1]
}

search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=1)
search_result = search.fit(X_train_reshaped, y_train)

print("Best: %f using %s" % (search_result.best_score_, search_result.best_params_))

# Predict on test set
best_model = search.best_estimator_.model
y_pred = best_model.predict(X_test_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Print classification report
print(classification_report(y_true, y_pred_classes))

"""# eye tracking arousal"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Add, GlobalAveragePooling1D, Dense, LSTM
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.metrics import classification_report
arousal_df = df_eye_tracking[['Quad_Cat', 'Num_of_Fixations', 'Mean_Fixation_Duration',
       'SD_Fixation_Duration', 'Skew_Fixation_Duration',
       'Max_Fixation_Duration', 'First_Fixation_Duration', 'Num_of_Saccade',
       'Mean_Saccade_Duration', 'SD_Saccade_Duration', 'Skew_Saccade_Duration',
       'Max_Saccade_Duration', 'Mean_Saccade_Amplitude',
       'SD_Saccade_Amplitude', 'Skew_Saccade_Amplitude',
       'Max_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'SD_Saccade_Direction', 'Skew_Saccade_Direction',
       'Max_Saccade_Direction', 'Mean_Saccade_Length', 'SD_Saccade_Length',
       'Skew_Saccade_Length', 'Max_Saccade_Length', 'Num_of_Blink',
       'Mean_Blink_Duration', 'SD_Blink_Duration', 'Skew_Blink_Duration',
       'Max_Blink_Duration', 'Num_of_Microsac', 'Mean_Microsac_Peak_Vel',
       'SD_Microsac_Peak_Vel', 'Skew_Microsac_Peak_Vel',
       'Max_Microsac_Peak_Vel', 'Mean_Microsac_Ampl', 'SD_Microsac_Ampl',
       'Skew_Microsac_Ampl', 'Max_Microsac_Ampl', 'Mean_Microsac_Dir',
       'SD_Microsac_Dir', 'Skew_Microsac_Dir', 'Max_Microsac_Dir',
       'Mean_Microsac_H_Amp', 'SD_Microsac_H_Amp', 'Skew_Microsac_H_Amp',
       'Max_Microsac_H_Amp', 'Mean_Microsac_V_Amp', 'SD_Microsac_V_Amp',
       'Skew_Microsac_V_Amp', 'Max_Microsac_V_Amp']].copy()

arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)

X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']

scaler = StandardScaler()
X_arousal_scaled = scaler.fit_transform(X_arousal)
X_arousal_reshaped = X_arousal_scaled.reshape(-1, 1, X_arousal_scaled.shape[1])

encoder = LabelEncoder()
encoder.fit(y_arousal)
encoded_y = encoder.transform(y_arousal)
y_arousal_categorical = to_categorical(encoded_y)



# No need to reshape X_arousal_scaled here
X_train, X_test, y_train, y_test = train_test_split(X_arousal_scaled, y_arousal_categorical, test_size=0.1, random_state=42)

# Reshape data
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)



input_shape = X_train_reshaped.shape[1:]
#num_classes = y_categorical.shape[1]
num_classes = y_arousal_categorical.shape[1]

# ResNet block
def resnet_block(input_data, filters, conv_size):
    x = Conv1D(filters, conv_size, activation='relu', padding='same')(input_data)
    x = BatchNormalization()(x)
    x = Conv1D(filters, conv_size, activation=None, padding='same')(x)
    x = BatchNormalization()(x)


    if input_data.shape[-1] != filters:
        input_data = Conv1D(filters, 1, activation=None, padding='same')(input_data)
        input_data = BatchNormalization()(input_data)

    x = Add()([x, input_data])
    x = Activation('relu')(x)
    return x

# Hybrid ResNet-LSTM model
def build_resnet_lstm_model(input_shape, num_classes, filters=32, lstm_units=50):
    inputs = Input(shape=input_shape)

    x = Conv1D(16, 3, activation='relu')(inputs)
    x = Conv1D(32, 3, activation='relu')(x)
    x = BatchNormalization()(x)

    x = resnet_block(x, filters, 3)
    x = resnet_block(x, filters, 3)

    x = LSTM(lstm_units, return_sequences=True)(x)
    x = LSTM(lstm_units)(x)

    #outputs = Dense(num_classes, activation='softmax')(x)
    outputs = Dense(2, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Model with hyperparameter configurations
def create_model(filters=32, lstm_units=50, learning_rate=0.001):
    model = build_resnet_lstm_model(input_shape, num_classes, filters=filters, lstm_units=lstm_units)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)

param_grid = {
    'filters': [16, 32, 64],
    'lstm_units': [30, 50, 70],
    'learning_rate': [0.001, 0.01, 0.1]
}

search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=1)
search_result = search.fit(X_train_reshaped, y_train)

print("Best: %f using %s" % (search_result.best_score_, search_result.best_params_))

# Predict on test set
best_model = search.best_estimator_.model
y_pred = best_model.predict(X_test_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Print classification report
print(classification_report(y_true, y_pred_classes))

"""# eye tracking valence"""

valence_df = df_eye_tracking[['Quad_Cat', 'Num_of_Fixations', 'Mean_Fixation_Duration',
       'SD_Fixation_Duration', 'Skew_Fixation_Duration',
       'Max_Fixation_Duration', 'First_Fixation_Duration', 'Num_of_Saccade',
       'Mean_Saccade_Duration', 'SD_Saccade_Duration', 'Skew_Saccade_Duration',
       'Max_Saccade_Duration', 'Mean_Saccade_Amplitude',
       'SD_Saccade_Amplitude', 'Skew_Saccade_Amplitude',
       'Max_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'SD_Saccade_Direction', 'Skew_Saccade_Direction',
       'Max_Saccade_Direction', 'Mean_Saccade_Length', 'SD_Saccade_Length',
       'Skew_Saccade_Length', 'Max_Saccade_Length', 'Num_of_Blink',
       'Mean_Blink_Duration', 'SD_Blink_Duration', 'Skew_Blink_Duration',
       'Max_Blink_Duration', 'Num_of_Microsac', 'Mean_Microsac_Peak_Vel',
       'SD_Microsac_Peak_Vel', 'Skew_Microsac_Peak_Vel',
       'Max_Microsac_Peak_Vel', 'Mean_Microsac_Ampl', 'SD_Microsac_Ampl',
       'Skew_Microsac_Ampl', 'Max_Microsac_Ampl', 'Mean_Microsac_Dir',
       'SD_Microsac_Dir', 'Skew_Microsac_Dir', 'Max_Microsac_Dir',
       'Mean_Microsac_H_Amp', 'SD_Microsac_H_Amp', 'Skew_Microsac_H_Amp',
       'Max_Microsac_H_Amp', 'Mean_Microsac_V_Amp', 'SD_Microsac_V_Amp',
       'Skew_Microsac_V_Amp', 'Max_Microsac_V_Amp']].copy()

valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)

X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']

scaler = StandardScaler()
X_valence_scaled = scaler.fit_transform(
X_valence)
X_valence_reshaped = X_valence_scaled.reshape(-1, 1, X_valence_scaled.shape[1])

encoder = LabelEncoder()
encoder.fit(y_valence)
encoded_y = encoder.transform(y_valence)
y_valence_categorical = to_categorical(encoded_y)





X_train, X_test, y_train, y_test = train_test_split(X_valence_scaled, y_valence_categorical, test_size=0.1, random_state=42)

# Reshape data
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)



input_shape = X_train_reshaped.shape[1:]
#num_classes = y_categorical.shape[1]
num_classes = y_valence_categorical.shape[1]

# ResNet block
def resnet_block(input_data, filters, conv_size):
    x = Conv1D(filters, conv_size, activation='relu', padding='same')(input_data)
    x = BatchNormalization()(x)
    x = Conv1D(filters, conv_size, activation=None, padding='same')(x)
    x = BatchNormalization()(x)


    if input_data.shape[-1] != filters:
        input_data = Conv1D(filters, 1, activation=None, padding='same')(input_data)
        input_data = BatchNormalization()(input_data)

    x = Add()([x, input_data])
    x = Activation('relu')(x)
    return x

# Hybrid ResNet-LSTM model
def build_resnet_lstm_model(input_shape, num_classes, filters=32, lstm_units=50):
    inputs = Input(shape=input_shape)

    x = Conv1D(16, 3, activation='relu')(inputs)
    x = Conv1D(32, 3, activation='relu')(x)
    x = BatchNormalization()(x)

    x = resnet_block(x, filters, 3)
    x = resnet_block(x, filters, 3)

    x = LSTM(lstm_units, return_sequences=True)(x)
    x = LSTM(lstm_units)(x)

    #outputs = Dense(num_classes, activation='softmax')(x)
    outputs = Dense(2, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Model with hyperparameter configurations
def create_model(filters=32, lstm_units=50, learning_rate=0.001):
    model = build_resnet_lstm_model(input_shape, num_classes, filters=filters, lstm_units=lstm_units)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)

param_grid = {
    'filters': [16, 32, 64],
    'lstm_units': [30, 50, 70],
    'learning_rate': [0.001, 0.01, 0.1]
}

search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=1)
search_result = search.fit(X_train_reshaped, y_train)

print("Best: %f using %s" % (search_result.best_score_, search_result.best_params_))

# Predict on test set
best_model = search.best_estimator_.model
y_pred = best_model.predict(X_test_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Print classification report
print(classification_report(y_true, y_pred_classes))

"""# All data 4 class"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Add, GlobalAveragePooling1D, Dense, LSTM
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.metrics import classification_report


X = df_all_data.drop('Quad_Cat', axis=1).values
y = df_all_data['Quad_Cat'].values

# Scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Converting labels to integer then to categorical
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)
y_categorical = to_categorical(y_encoded)

# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_categorical, test_size=0.1, random_state=42)

# Reshape data
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

input_shape = X_train_reshaped.shape[1:]
num_classes = y_categorical.shape[1]

# ResNet block
def resnet_block(input_data, filters, conv_size):
    x = Conv1D(filters, conv_size, activation='relu', padding='same')(input_data)
    x = BatchNormalization()(x)
    x = Conv1D(filters, conv_size, activation=None, padding='same')(x)
    x = BatchNormalization()(x)


    if input_data.shape[-1] != filters:
        input_data = Conv1D(filters, 1, activation=None, padding='same')(input_data)
        input_data = BatchNormalization()(input_data)

    x = Add()([x, input_data])
    x = Activation('relu')(x)
    return x

# Hybrid ResNet-LSTM model
def build_resnet_lstm_model(input_shape, num_classes, filters=32, lstm_units=50):
    inputs = Input(shape=input_shape)

    x = Conv1D(16, 3, activation='relu')(inputs)
    x = Conv1D(32, 3, activation='relu')(x)
    x = BatchNormalization()(x)

    x = resnet_block(x, filters, 3)
    x = resnet_block(x, filters, 3)

    x = LSTM(lstm_units, return_sequences=True)(x)
    x = LSTM(lstm_units)(x)

    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Model with hyperparameter configurations
def create_model(filters=32, lstm_units=50, learning_rate=0.001):
    model = build_resnet_lstm_model(input_shape, num_classes, filters=filters, lstm_units=lstm_units)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)

param_grid = {
    'filters': [16, 32, 64],
    'lstm_units': [30, 50, 70],
    'learning_rate': [0.001, 0.01, 0.1]
}

search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=1)
search_result = search.fit(X_train_reshaped, y_train)

print("Best: %f using %s" % (search_result.best_score_, search_result.best_params_))

# Predict on test set
best_model = search.best_estimator_.model
y_pred = best_model.predict(X_test_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Print classification report
print(classification_report(y_true, y_pred_classes))

"""# all data arousal"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Add, GlobalAveragePooling1D, Dense, LSTM
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.metrics import classification_report
arousal_df = df_all_data[['Quad_Cat', 'Num_of_Fixations', 'Mean_Fixation_Duration',
       'SD_Fixation_Duration', 'Skew_Fixation_Duration',
       'Max_Fixation_Duration', 'First_Fixation_Duration', 'Num_of_Saccade',
       'Mean_Saccade_Duration', 'SD_Saccade_Duration', 'Skew_Saccade_Duration',
       'Max_Saccade_Duration', 'Mean_Saccade_Amplitude',
       'SD_Saccade_Amplitude', 'Skew_Saccade_Amplitude',
       'Max_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'SD_Saccade_Direction', 'Skew_Saccade_Direction',
       'Max_Saccade_Direction', 'Mean_Saccade_Length', 'SD_Saccade_Length',
       'Skew_Saccade_Length', 'Max_Saccade_Length', 'Num_of_Blink',
       'Mean_Blink_Duration', 'SD_Blink_Duration', 'Skew_Blink_Duration',
       'Max_Blink_Duration', 'Num_of_Microsac', 'Mean_Microsac_Peak_Vel',
       'SD_Microsac_Peak_Vel', 'Skew_Microsac_Peak_Vel',
       'Max_Microsac_Peak_Vel', 'Mean_Microsac_Ampl', 'SD_Microsac_Ampl',
       'Skew_Microsac_Ampl', 'Max_Microsac_Ampl', 'Mean_Microsac_Dir',
       'SD_Microsac_Dir', 'Skew_Microsac_Dir', 'Max_Microsac_Dir',
       'Mean_Microsac_H_Amp', 'SD_Microsac_H_Amp', 'Skew_Microsac_H_Amp',
       'Max_Microsac_H_Amp', 'Mean_Microsac_V_Amp', 'SD_Microsac_V_Amp',
       'Skew_Microsac_V_Amp', 'Max_Microsac_V_Amp', 'Mean', 'SD', 'Variance',
       'Minimum', 'Maximum', 'Number of Peaks', 'Number of Valleys', 'Ratio',
       'Mean.1', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR', 'MaxRR', 'LF',
       'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd', 'Pnn50', 'pnn20',
       'Pnn50pnn20']].copy()

arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)

X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']

scaler = StandardScaler()
X_arousal_scaled = scaler.fit_transform(X_arousal)
X_arousal_reshaped = X_arousal_scaled.reshape(-1, 1, X_arousal_scaled.shape[1])

encoder = LabelEncoder()
encoder.fit(y_arousal)
encoded_y = encoder.transform(y_arousal)
y_arousal_categorical = to_categorical(encoded_y)



X_train, X_test, y_train, y_test = train_test_split(X_arousal_scaled, y_arousal_categorical, test_size=0.1, random_state=42)

# Reshape data
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)



input_shape = X_train_reshaped.shape[1:]
#num_classes = y_categorical.shape[1]
num_classes = y_arousal_categorical.shape[1]

# ResNet block
def resnet_block(input_data, filters, conv_size):
    x = Conv1D(filters, conv_size, activation='relu', padding='same')(input_data)
    x = BatchNormalization()(x)
    x = Conv1D(filters, conv_size, activation=None, padding='same')(x)
    x = BatchNormalization()(x)


    if input_data.shape[-1] != filters:
        input_data = Conv1D(filters, 1, activation=None, padding='same')(input_data)
        input_data = BatchNormalization()(input_data)

    x = Add()([x, input_data])
    x = Activation('relu')(x)
    return x

# Hybrid ResNet-LSTM model
def build_resnet_lstm_model(input_shape, num_classes, filters=32, lstm_units=50):
    inputs = Input(shape=input_shape)

    x = Conv1D(16, 3, activation='relu')(inputs)
    x = Conv1D(32, 3, activation='relu')(x)
    x = BatchNormalization()(x)

    x = resnet_block(x, filters, 3)
    x = resnet_block(x, filters, 3)

    x = LSTM(lstm_units, return_sequences=True)(x)
    x = LSTM(lstm_units)(x)

    #outputs = Dense(num_classes, activation='softmax')(x)
    outputs = Dense(2, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Model with hyperparameter configurations
def create_model(filters=32, lstm_units=50, learning_rate=0.001):
    model = build_resnet_lstm_model(input_shape, num_classes, filters=filters, lstm_units=lstm_units)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)

param_grid = {
    'filters': [16, 32, 64],
    'lstm_units': [30, 50, 70],
    'learning_rate': [0.001, 0.01, 0.1]
}

search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=1)
search_result = search.fit(X_train_reshaped, y_train)

print("Best: %f using %s" % (search_result.best_score_, search_result.best_params_))

# Predict on test set
best_model = search.best_estimator_.model
y_pred = best_model.predict(X_test_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Print classification report
print(classification_report(y_true, y_pred_classes))

"""# all data valence"""

valence_df = df_all_data[['Quad_Cat', 'Num_of_Fixations', 'Mean_Fixation_Duration',
       'SD_Fixation_Duration', 'Skew_Fixation_Duration',
       'Max_Fixation_Duration', 'First_Fixation_Duration', 'Num_of_Saccade',
       'Mean_Saccade_Duration', 'SD_Saccade_Duration', 'Skew_Saccade_Duration',
       'Max_Saccade_Duration', 'Mean_Saccade_Amplitude',
       'SD_Saccade_Amplitude', 'Skew_Saccade_Amplitude',
       'Max_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'SD_Saccade_Direction', 'Skew_Saccade_Direction',
       'Max_Saccade_Direction', 'Mean_Saccade_Length', 'SD_Saccade_Length',
       'Skew_Saccade_Length', 'Max_Saccade_Length', 'Num_of_Blink',
       'Mean_Blink_Duration', 'SD_Blink_Duration', 'Skew_Blink_Duration',
       'Max_Blink_Duration', 'Num_of_Microsac', 'Mean_Microsac_Peak_Vel',
       'SD_Microsac_Peak_Vel', 'Skew_Microsac_Peak_Vel',
       'Max_Microsac_Peak_Vel', 'Mean_Microsac_Ampl', 'SD_Microsac_Ampl',
       'Skew_Microsac_Ampl', 'Max_Microsac_Ampl', 'Mean_Microsac_Dir',
       'SD_Microsac_Dir', 'Skew_Microsac_Dir', 'Max_Microsac_Dir',
       'Mean_Microsac_H_Amp', 'SD_Microsac_H_Amp', 'Skew_Microsac_H_Amp',
       'Max_Microsac_H_Amp', 'Mean_Microsac_V_Amp', 'SD_Microsac_V_Amp',
       'Skew_Microsac_V_Amp', 'Max_Microsac_V_Amp', 'Mean', 'SD', 'Variance',
       'Minimum', 'Maximum', 'Number of Peaks', 'Number of Valleys', 'Ratio',
       'Mean.1', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR', 'MaxRR', 'LF',
       'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd', 'Pnn50', 'pnn20',
       'Pnn50pnn20']].copy()

valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)

X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']

scaler = StandardScaler()
X_valence_scaled = scaler.fit_transform(
X_valence)
X_valence_reshaped = X_valence_scaled.reshape(-1, 1, X_valence_scaled.shape[1])

encoder = LabelEncoder()
encoder.fit(y_valence)
encoded_y = encoder.transform(y_valence)
y_valence_categorical = to_categorical(encoded_y)



X_train, X_test, y_train, y_test = train_test_split(X_valence_scaled, y_valence_categorical, test_size=0.1, random_state=42)

# Reshape data
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)



input_shape = X_train_reshaped.shape[1:]
#num_classes = y_categorical.shape[1]
num_classes = y_valence_categorical.shape[1]

# ResNet block
def resnet_block(input_data, filters, conv_size):
    x = Conv1D(filters, conv_size, activation='relu', padding='same')(input_data)
    x = BatchNormalization()(x)
    x = Conv1D(filters, conv_size, activation=None, padding='same')(x)
    x = BatchNormalization()(x)

    if input_data.shape[-1] != filters:
        input_data = Conv1D(filters, 1, activation=None, padding='same')(input_data)
        input_data = BatchNormalization()(input_data)

    x = Add()([x, input_data])
    x = Activation('relu')(x)
    return x

# Hybrid ResNet-LSTM model
def build_resnet_lstm_model(input_shape, num_classes, filters=32, lstm_units=50):
    inputs = Input(shape=input_shape)

    x = Conv1D(16, 3, activation='relu')(inputs)
    x = Conv1D(32, 3, activation='relu')(x)
    x = BatchNormalization()(x)

    x = resnet_block(x, filters, 3)
    x = resnet_block(x, filters, 3)

    x = LSTM(lstm_units, return_sequences=True)(x)
    x = LSTM(lstm_units)(x)

    #outputs = Dense(num_classes, activation='softmax')(x)
    outputs = Dense(2, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Model with hyperparameter configurations
def create_model(filters=32, lstm_units=50, learning_rate=0.001):
    model = build_resnet_lstm_model(input_shape, num_classes, filters=filters, lstm_units=lstm_units)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)

param_grid = {
    'filters': [16, 32, 64],
    'lstm_units': [30, 50, 70],
    'learning_rate': [0.001, 0.01, 0.1]
}

search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=1)
search_result = search.fit(X_train_reshaped, y_train)

print("Best: %f using %s" % (search_result.best_score_, search_result.best_params_))

# Predict on test set
best_model = search.best_estimator_.model
y_pred = best_model.predict(X_test_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Print classification report
print(classification_report(y_true, y_pred_classes))