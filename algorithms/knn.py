# -*- coding: utf-8 -*-
"""KNN classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KZ1vEOuvDFeDBPBVJ-qkFmQfBIIINGbX

# Importing necessary libraries
"""

import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

"""# Importing datasets"""

df_gcr = pd.read_csv('GSR_FeaturesExtracted.csv')
df_ecg = pd.read_csv('ECG_FeaturesExtracted.csv')
df_eye_tracking = pd.read_csv('EyeTracking_FeaturesExtracted.csv')

df_all_data = pd.read_csv('all_data.csv')

df_eye_tracking.shape

"""# KNN -GSR for arousal and valence"""

# We Subset the dataset for arousal classification
arousal_df = df_gcr[['Mean', 'SD', 'Variance', 'Minimum', 'Maximum', 'Number of Peaks', 'Number of Valleys', 'Ratio', 'Quad_Cat']]

# We Subset the dataset for valence classification
valence_df = df_gcr[['Mean', 'SD', 'Variance', 'Minimum', 'Maximum', 'Number of Peaks', 'Number of Valleys', 'Ratio', 'Quad_Cat']]

# We Map the Quad_Cat values to respective arousal and valence labels
arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}

arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)

# Spliting the arousal dataset into feature matrix X_arousal and target vector y_arousal
X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']

# Spliting the valence dataset into feature matrix X_valence and target vector y_valence
X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']

# Spliting the arousal data into training and testing sets
X_arousal_train, X_arousal_test, y_arousal_train, y_arousal_test = train_test_split(X_arousal, y_arousal, test_size=0.1, random_state=42)

# Spliting the valence data into training and testing sets
X_valence_train, X_valence_test, y_valence_train, y_valence_test = train_test_split(X_valence, y_valence, test_size=0.1, random_state=42)

# a pipeline with preprocessing and KNN classifier
pipeline_arousal = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step: Center and scale
    ('knn', KNeighborsClassifier())  # KNN classifier
])

pipeline_valence = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step: Center and scale
    ('knn', KNeighborsClassifier())  # KNN classifier
])

# the hyperparameter grid for KNN
param_grid = {
    'knn__n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors
    'knn__weights': ['uniform', 'distance']  # Weighting scheme
}

# grid search with 10-fold cross-validation for arousal
grid_search_arousal = GridSearchCV(pipeline_arousal, param_grid, cv=10, scoring='accuracy', return_train_score=True)
grid_search_arousal.fit(X_arousal_train, y_arousal_train)

# grid search with 10-fold cross-validation for valence
grid_search_valence = GridSearchCV(pipeline_valence, param_grid, cv=10, scoring='accuracy', return_train_score=True)
grid_search_valence.fit(X_valence_train, y_valence_train)

# predictions on the arousal test set
y_arousal_pred = grid_search_arousal.predict(X_arousal_test)

# predictions on the valence test set
y_valence_pred = grid_search_valence.predict(X_valence_test)

# evaluation metrics for arousal
accuracy_arousal = accuracy_score(y_arousal_test, y_arousal_pred)
precision_arousal = precision_score(y_arousal_test, y_arousal_pred, average='weighted')
recall_arousal = recall_score(y_arousal_test, y_arousal_pred, average='weighted')
f1_arousal = f1_score(y_arousal_test, y_arousal_pred, average='weighted')

print("Arousal Metrics:")
print("Accuracy:", accuracy_arousal)
print("Precision:", precision_arousal)
print("Recall:", recall_arousal)
print("F1-score:", f1_arousal)
print()

# evaluation metrics for valence
accuracy_valence = accuracy_score(y_valence_test, y_valence_pred)
precision_valence = precision_score(y_valence_test, y_valence_pred, average='weighted')
recall_valence = recall_score(y_valence_test, y_valence_pred, average='weighted')
f1_valence = f1_score(y_valence_test, y_valence_pred, average='weighted')

print("Valence Metrics:")
print("Accuracy:", accuracy_valence)
print("Precision:", precision_valence)
print("Recall:", recall_valence)
print("F1-score:", f1_valence)

from keras.utils import plot_model
plot_model(grid_search, to_file = 'model_plot.png', show_shapes = True, show_layer_names=True)

"""# KNN - GSR"""

# we Subset the dataset for the features and target
X = df_gcr[['Mean', 'SD', 'Variance', 'Minimum', 'Maximum', 'Number of Peaks', 'Number of Valleys', 'Ratio']]
y = df_gcr['Quad_Cat']

# We Map the Quad_Cat values to respective labels
label_mapping = {0: 'High Arousal High Valence', 1: 'Low Arousal High Valence', 2: 'Low Arousal Low Valence', 3: 'High Arousal Low Valence'}

y = y.map(label_mapping)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# a pipeline with preprocessing and KNN classifier
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step: Center and scale
    ('knn', KNeighborsClassifier())  # KNN classifier
])

# the hyperparameter grid for KNN
param_grid = {
    'knn__n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors
    'knn__weights': ['uniform', 'distance']  # Weighting scheme
}

# grid search with 10-fold cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy', return_train_score=True)
grid_search.fit(X_train, y_train)

# predictions on the test set
y_pred = grid_search.predict(X_test)

# evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Metrics:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

"""# KNN - ECG for valence and arousal"""

# we Subset the dataset for valence classification
valence_df = df_ecg[['Mean', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR', 'MaxRR', 'LF', 'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd', 'Pnn50', 'pnn20', 'Pnn50pnn20', 'Quad_Cat']]

# we Subset the dataset for arousal classification
arousal_df = df_ecg[['Mean', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR', 'MaxRR', 'LF', 'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd', 'Pnn50', 'pnn20', 'Pnn50pnn20', 'Quad_Cat']]
# We Map the Quad_Cat values to respective arousal and valence labels
arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}

arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)

# Spliting the arousal dataset into feature matrix X_arousal and target vector y_arousal
X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']

# Spliting the valence dataset into feature matrix X_valence and target vector y_valence
X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']

# Spliting the arousal data into training and testing sets
X_arousal_train, X_arousal_test, y_arousal_train, y_arousal_test = train_test_split(X_arousal, y_arousal, test_size=0.1, random_state=42)

# Spliting the valence data into training and testing sets
X_valence_train, X_valence_test, y_valence_train, y_valence_test = train_test_split(X_valence, y_valence, test_size=0.1, random_state=42)

# a pipeline with preprocessing and KNN classifier
pipeline_arousal = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step: Center and scale
    ('knn', KNeighborsClassifier())  # KNN classifier
])

pipeline_valence = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step: Center and scale
    ('knn', KNeighborsClassifier())  # KNN classifier
])

# the hyperparameter grid for KNN
param_grid = {
    'knn__n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors
    'knn__weights': ['uniform', 'distance']  # Weighting scheme
}

# grid search with 10-fold cross-validation for arousal
grid_search_arousal = GridSearchCV(pipeline_arousal, param_grid, cv=10, scoring='accuracy', return_train_score=True)
grid_search_arousal.fit(X_arousal_train, y_arousal_train)

# grid search with 10-fold cross-validation for valence
grid_search_valence = GridSearchCV(pipeline_valence, param_grid, cv=10, scoring='accuracy', return_train_score=True)
grid_search_valence.fit(X_valence_train, y_valence_train)

# predictions on the arousal test set
y_arousal_pred = grid_search_arousal.predict(X_arousal_test)

# predictions on the valence test set
y_valence_pred = grid_search_valence.predict(X_valence_test)

# evaluation metrics for arousal
accuracy_arousal = accuracy_score(y_arousal_test, y_arousal_pred)
precision_arousal = precision_score(y_arousal_test, y_arousal_pred, average='weighted')
recall_arousal = recall_score(y_arousal_test, y_arousal_pred, average='weighted')
f1_arousal = f1_score(y_arousal_test, y_arousal_pred, average='weighted')

print("Arousal Metrics:")
print("Accuracy:", accuracy_arousal)
print("Precision:", precision_arousal)
print("Recall:", recall_arousal)
print("F1-score:", f1_arousal)
print()

# evaluation metrics for valence
accuracy_valence = accuracy_score(y_valence_test, y_valence_pred)
precision_valence = precision_score(y_valence_test, y_valence_pred, average='weighted')
recall_valence = recall_score(y_valence_test, y_valence_pred, average='weighted')
f1_valence = f1_score(y_valence_test, y_valence_pred, average='weighted')

print("Valence Metrics:")
print("Accuracy:", accuracy_valence)
print("Precision:", precision_valence)
print("Recall:", recall_valence)
print("F1-score:", f1_valence)

"""# KNN - ECG"""

# we Subset the dataset for the features and target
X = df_ecg[['Mean', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR', 'MaxRR', 'LF', 'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd', 'Pnn50', 'pnn20', 'Pnn50pnn20']]
y = df_ecg['Quad_Cat']

# We Map the Quad_Cat values to respective labels
label_mapping = {0: 'High Arousal High Valence', 1: 'Low Arousal High Valence', 2: 'Low Arousal Low Valence', 3: 'High Arousal Low Valence'}

y = y.map(label_mapping)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# a pipeline with preprocessing and KNN classifier
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step: Center and scale
    ('knn', KNeighborsClassifier())  # KNN classifier
])

# the hyperparameter grid for KNN
param_grid = {
    'knn__n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors
    'knn__weights': ['uniform', 'distance']  # Weighting scheme
}

# grid search with 10-fold cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy', return_train_score=True)
grid_search.fit(X_train, y_train)

# predictions on the test set
y_pred = grid_search.predict(X_test)

# evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Metrics:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

"""# KNN EYE Tracking for arousal and valence"""

### we drop NAN values from eye tracking data
# Drop rows with NaN values
df_eye_tracking.dropna(inplace=True)

# Drop columns with NaN values
df_eye_tracking.dropna(axis=1, inplace=True)

# we Subset the dataset for valence classification
valence_df = df_eye_tracking[['Quad_Cat', 'Num_of_Fixations', 'Mean_Fixation_Duration',
       'SD_Fixation_Duration', 'Skew_Fixation_Duration',
       'Max_Fixation_Duration', 'First_Fixation_Duration', 'Num_of_Saccade',
       'Mean_Saccade_Duration', 'SD_Saccade_Duration', 'Skew_Saccade_Duration',
       'Max_Saccade_Duration', 'Mean_Saccade_Amplitude',
       'SD_Saccade_Amplitude', 'Skew_Saccade_Amplitude',
       'Max_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'SD_Saccade_Direction', 'Skew_Saccade_Direction',
       'Max_Saccade_Direction', 'Mean_Saccade_Length', 'SD_Saccade_Length',
       'Skew_Saccade_Length', 'Max_Saccade_Length', 'Num_of_Blink',
       'Mean_Blink_Duration', 'SD_Blink_Duration', 'Skew_Blink_Duration',
       'Max_Blink_Duration', 'Num_of_Microsac', 'Mean_Microsac_Peak_Vel',
       'SD_Microsac_Peak_Vel', 'Skew_Microsac_Peak_Vel',
       'Max_Microsac_Peak_Vel', 'Mean_Microsac_Ampl', 'SD_Microsac_Ampl',
       'Skew_Microsac_Ampl', 'Max_Microsac_Ampl', 'Mean_Microsac_Dir',
       'SD_Microsac_Dir', 'Skew_Microsac_Dir', 'Max_Microsac_Dir',
       'Mean_Microsac_H_Amp', 'SD_Microsac_H_Amp', 'Skew_Microsac_H_Amp',
       'Max_Microsac_H_Amp', 'Mean_Microsac_V_Amp', 'SD_Microsac_V_Amp',
       'Skew_Microsac_V_Amp', 'Max_Microsac_V_Amp']]

# we Subset the dataset for arousal classification
arousal_df = df_eye_tracking[['Quad_Cat', 'Num_of_Fixations', 'Mean_Fixation_Duration',
       'SD_Fixation_Duration', 'Skew_Fixation_Duration',
       'Max_Fixation_Duration', 'First_Fixation_Duration', 'Num_of_Saccade',
       'Mean_Saccade_Duration', 'SD_Saccade_Duration', 'Skew_Saccade_Duration',
       'Max_Saccade_Duration', 'Mean_Saccade_Amplitude',
       'SD_Saccade_Amplitude', 'Skew_Saccade_Amplitude',
       'Max_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'SD_Saccade_Direction', 'Skew_Saccade_Direction',
       'Max_Saccade_Direction', 'Mean_Saccade_Length', 'SD_Saccade_Length',
       'Skew_Saccade_Length', 'Max_Saccade_Length', 'Num_of_Blink',
       'Mean_Blink_Duration', 'SD_Blink_Duration', 'Skew_Blink_Duration',
       'Max_Blink_Duration', 'Num_of_Microsac', 'Mean_Microsac_Peak_Vel',
       'SD_Microsac_Peak_Vel', 'Skew_Microsac_Peak_Vel',
       'Max_Microsac_Peak_Vel', 'Mean_Microsac_Ampl', 'SD_Microsac_Ampl',
       'Skew_Microsac_Ampl', 'Max_Microsac_Ampl', 'Mean_Microsac_Dir',
       'SD_Microsac_Dir', 'Skew_Microsac_Dir', 'Max_Microsac_Dir',
       'Mean_Microsac_H_Amp', 'SD_Microsac_H_Amp', 'Skew_Microsac_H_Amp',
       'Max_Microsac_H_Amp', 'Mean_Microsac_V_Amp', 'SD_Microsac_V_Amp',
       'Skew_Microsac_V_Amp', 'Max_Microsac_V_Amp']]
# We Map the Quad_Cat values to respective arousal and valence labels
arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}

arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)

# Spliting the arousal dataset into feature matrix X_arousal and target vector y_arousal
X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']

# Spliting the valence dataset into feature matrix X_valence and target vector y_valence
X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']

# Spliting the arousal data into training and testing sets
X_arousal_train, X_arousal_test, y_arousal_train, y_arousal_test = train_test_split(X_arousal, y_arousal, test_size=0.1, random_state=42)

# Spliting the valence data into training and testing sets
X_valence_train, X_valence_test, y_valence_train, y_valence_test = train_test_split(X_valence, y_valence, test_size=0.1, random_state=42)

# a pipeline with preprocessing and KNN classifier
pipeline_arousal = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step: Center and scale
    ('knn', KNeighborsClassifier())  # KNN classifier
])

pipeline_valence = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step: Center and scale
    ('knn', KNeighborsClassifier())  # KNN classifier
])

# the hyperparameter grid for KNN
param_grid = {
    'knn__n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors
    'knn__weights': ['uniform', 'distance']  # Weighting scheme
}

# grid search with 10-fold cross-validation for arousal
grid_search_arousal = GridSearchCV(pipeline_arousal, param_grid, cv=10, scoring='accuracy', return_train_score=True)
grid_search_arousal.fit(X_arousal_train, y_arousal_train)

# grid search with 10-fold cross-validation for valence
grid_search_valence = GridSearchCV(pipeline_valence, param_grid, cv=10, scoring='accuracy', return_train_score=True)
grid_search_valence.fit(X_valence_train, y_valence_train)

# predictions on the arousal test set
y_arousal_pred = grid_search_arousal.predict(X_arousal_test)

# predictions on the valence test set
y_valence_pred = grid_search_valence.predict(X_valence_test)

# evaluation metrics for arousal
accuracy_arousal = accuracy_score(y_arousal_test, y_arousal_pred)
precision_arousal = precision_score(y_arousal_test, y_arousal_pred, average='weighted')
recall_arousal = recall_score(y_arousal_test, y_arousal_pred, average='weighted')
f1_arousal = f1_score(y_arousal_test, y_arousal_pred, average='weighted')

print("Arousal Metrics:")
print("Accuracy:", accuracy_arousal)
print("Precision:", precision_arousal)
print("Recall:", recall_arousal)
print("F1-score:", f1_arousal)
print()

# evaluation metrics for valence
accuracy_valence = accuracy_score(y_valence_test, y_valence_pred)
precision_valence = precision_score(y_valence_test, y_valence_pred, average='weighted')
recall_valence = recall_score(y_valence_test, y_valence_pred, average='weighted')
f1_valence = f1_score(y_valence_test, y_valence_pred, average='weighted')

print("Valence Metrics:")
print("Accuracy:", accuracy_valence)
print("Precision:", precision_valence)
print("Recall:", recall_valence)
print("F1-score:", f1_valence)

"""# KNN Eye Tracking"""

# we Subset the dataset for the features and target
X = df_eye_tracking[['Quad_Cat', 'Num_of_Fixations', 'Mean_Fixation_Duration',
       'SD_Fixation_Duration', 'Skew_Fixation_Duration',
       'Max_Fixation_Duration', 'First_Fixation_Duration', 'Num_of_Saccade',
       'Mean_Saccade_Duration', 'SD_Saccade_Duration', 'Skew_Saccade_Duration',
       'Max_Saccade_Duration', 'Mean_Saccade_Amplitude',
       'SD_Saccade_Amplitude', 'Skew_Saccade_Amplitude',
       'Max_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'SD_Saccade_Direction', 'Skew_Saccade_Direction',
       'Max_Saccade_Direction', 'Mean_Saccade_Length', 'SD_Saccade_Length',
       'Skew_Saccade_Length', 'Max_Saccade_Length', 'Num_of_Blink',
       'Mean_Blink_Duration', 'SD_Blink_Duration', 'Skew_Blink_Duration',
       'Max_Blink_Duration', 'Num_of_Microsac', 'Mean_Microsac_Peak_Vel',
       'SD_Microsac_Peak_Vel', 'Skew_Microsac_Peak_Vel',
       'Max_Microsac_Peak_Vel', 'Mean_Microsac_Ampl', 'SD_Microsac_Ampl',
       'Skew_Microsac_Ampl', 'Max_Microsac_Ampl', 'Mean_Microsac_Dir',
       'SD_Microsac_Dir', 'Skew_Microsac_Dir', 'Max_Microsac_Dir',
       'Mean_Microsac_H_Amp', 'SD_Microsac_H_Amp', 'Skew_Microsac_H_Amp',
       'Max_Microsac_H_Amp', 'Mean_Microsac_V_Amp', 'SD_Microsac_V_Amp',
       'Skew_Microsac_V_Amp', 'Max_Microsac_V_Amp']]
y = df_eye_tracking['Quad_Cat']

# We Map the Quad_Cat values to respective labels
label_mapping = {0: 'High Arousal High Valence', 1: 'Low Arousal High Valence', 2: 'Low Arousal Low Valence', 3: 'High Arousal Low Valence'}

y = y.map(label_mapping)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# a pipeline with preprocessing and KNN classifier
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step: Center and scale
    ('knn', KNeighborsClassifier())  # KNN classifier
])

# the hyperparameter grid for KNN
param_grid = {
    'knn__n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors
    'knn__weights': ['uniform', 'distance']  # Weighting scheme
}

# grid search with 10-fold cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy', return_train_score=True)
grid_search.fit(X_train, y_train)

# predictions on the test set
y_pred = grid_search.predict(X_test)

# evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Metrics:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

"""# KNN ECG + GSR For arousal and valence"""

df_gsr_ecg = pd.read_csv('GSR+ECG.csv')

df_gsr_ecg.shape

df_gsr_ecg.columns

# we Subset the dataset for valence classification
valence_df = df_gsr_ecg[['Quad_Cat', 'Mean', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR',
       'MaxRR', 'LF', 'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd',
       'Pnn50', 'pnn20', 'Pnn50pnn20', 'Mean.1', 'SD', 'Variance', 'Minimum',
       'Maximum', 'Number of Peaks', 'Number of Valleys', 'Ratio']]

# we Subset the dataset for arousal classification
arousal_df = df_gsr_ecg[['Quad_Cat', 'Mean', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR',
       'MaxRR', 'LF', 'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd',
       'Pnn50', 'pnn20', 'Pnn50pnn20', 'Mean.1', 'SD', 'Variance', 'Minimum',
       'Maximum', 'Number of Peaks', 'Number of Valleys', 'Ratio']]
# We Map the Quad_Cat values to respective arousal and valence labels
arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}

arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)

# Spliting the arousal dataset into feature matrix X_arousal and target vector y_arousal
X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']

# Spliting the valence dataset into feature matrix X_valence and target vector y_valence
X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']

# Spliting the arousal data into training and testing sets
X_arousal_train, X_arousal_test, y_arousal_train, y_arousal_test = train_test_split(X_arousal, y_arousal, test_size=0.1, random_state=42)

# Spliting the valence data into training and testing sets
X_valence_train, X_valence_test, y_valence_train, y_valence_test = train_test_split(X_valence, y_valence, test_size=0.1, random_state=42)

# a pipeline with preprocessing and KNN classifier
pipeline_arousal = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step: Center and scale
    ('knn', KNeighborsClassifier())  # KNN classifier
])

pipeline_valence = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step: Center and scale
    ('knn', KNeighborsClassifier())  # KNN classifier
])

# the hyperparameter grid for KNN
param_grid = {
    'knn__n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors
    'knn__weights': ['uniform', 'distance']  # Weighting scheme
}

# grid search with 10-fold cross-validation for arousal
grid_search_arousal = GridSearchCV(pipeline_arousal, param_grid, cv=10, scoring='accuracy', return_train_score=True)
grid_search_arousal.fit(X_arousal_train, y_arousal_train)

# grid search with 10-fold cross-validation for valence
grid_search_valence = GridSearchCV(pipeline_valence, param_grid, cv=10, scoring='accuracy', return_train_score=True)
grid_search_valence.fit(X_valence_train, y_valence_train)

# predictions on the arousal test set
y_arousal_pred = grid_search_arousal.predict(X_arousal_test)

# predictions on the valence test set
y_valence_pred = grid_search_valence.predict(X_valence_test)

# evaluation metrics for arousal
accuracy_arousal = accuracy_score(y_arousal_test, y_arousal_pred)
precision_arousal = precision_score(y_arousal_test, y_arousal_pred, average='weighted')
recall_arousal = recall_score(y_arousal_test, y_arousal_pred, average='weighted')
f1_arousal = f1_score(y_arousal_test, y_arousal_pred, average='weighted')

print("Arousal Metrics:")
print("Accuracy:", accuracy_arousal)
print("Precision:", precision_arousal)
print("Recall:", recall_arousal)
print("F1-score:", f1_arousal)
print()

# evaluation metrics for valence
accuracy_valence = accuracy_score(y_valence_test, y_valence_pred)
precision_valence = precision_score(y_valence_test, y_valence_pred, average='weighted')
recall_valence = recall_score(y_valence_test, y_valence_pred, average='weighted')
f1_valence = f1_score(y_valence_test, y_valence_pred, average='weighted')

print("Valence Metrics:")
print("Accuracy:", accuracy_valence)
print("Precision:", precision_valence)
print("Recall:", recall_valence)
print("F1-score:", f1_valence)

"""# KNN GSR + ECG"""

X = df_gsr_ecg[['Quad_Cat', 'Mean', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR',
       'MaxRR', 'LF', 'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd',
       'Pnn50', 'pnn20', 'Pnn50pnn20', 'Mean.1', 'SD', 'Variance', 'Minimum',
       'Maximum', 'Number of Peaks', 'Number of Valleys', 'Ratio']]
y = df_gsr_ecg['Quad_Cat']

# We Map the Quad_Cat values to respective labels
label_mapping = {0: 'High Arousal High Valence', 1: 'Low Arousal High Valence', 2: 'Low Arousal Low Valence', 3: 'High Arousal Low Valence'}

y = y.map(label_mapping)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# a pipeline with preprocessing and KNN classifier
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step: Center and scale
    ('knn', KNeighborsClassifier())  # KNN classifier
])

# the hyperparameter grid for KNN
param_grid = {
    'knn__n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors
    'knn__weights': ['uniform', 'distance']  # Weighting scheme
}

# grid search with 10-fold cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy', return_train_score=True)
grid_search.fit(X_train, y_train)

# predictions on the test set
y_pred = grid_search.predict(X_test)

# evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Metrics:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

"""### KNN ECG + GSR + EYE Tracking for arousal and valence"""

### we drop NAN values from eye tracking data
# Drop rows with NaN values
df_eye_tracking.dropna(inplace=True)

# Drop columns with NaN values
df_eye_tracking.dropna(axis=1, inplace=True)

#we combine the dataframes again
# Concatenate the dataframes vertically
all_concatenated_df = pd.concat([concatenated_df, df_eye_tracking])

# Replacing NaN values in the first dataframe with non-null values from the second dataframe
all_concatenated_df = all_concatenated_df.combine_first(df_eye_tracking)

# Replacing NaN values in the second dataframe with non-null values from the first dataframe
all_concatenated_df = all_concatenated_df.combine_first(concatenated_df)

# we Reset the index of the concatenated dataframe
all_concatenated_df = all_concatenated_df.reset_index(drop=True)

all_concatenated_df.columns

all_concatenated_df.shape

all_concatenated_df.dropna(inplace=True)

# we Drop columns with NaN values
all_concatenated_df.dropna(axis=1, inplace=True)

# we Subset the dataset for valence classification
valence_df = all_concatenated_df[['Bpm', 'First_Fixation_Duration', 'HF', 'Ibi', 'LF', 'Max', 'MaxRR',
       'Max_Blink_Duration', 'Max_Fixation_Duration', 'Max_Microsac_Ampl',
       'Max_Microsac_Dir', 'Max_Microsac_H_Amp', 'Max_Microsac_Peak_Vel',
       'Max_Microsac_V_Amp', 'Max_Saccade_Amplitude', 'Max_Saccade_Direction',
       'Max_Saccade_Duration', 'Max_Saccade_Length', 'Maximum', 'Mean',
       'MeanRR', 'Mean_Blink_Duration', 'Mean_Fixation_Duration',
       'Mean_Microsac_Ampl', 'Mean_Microsac_Dir', 'Mean_Microsac_H_Amp',
       'Mean_Microsac_Peak_Vel', 'Mean_Microsac_V_Amp',
       'Mean_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'Mean_Saccade_Duration', 'Mean_Saccade_Length', 'MedianRR', 'Min',
       'MinRR', 'Minimum', 'Num_of_Blink', 'Num_of_Fixations',
       'Num_of_Microsac', 'Num_of_Saccade', 'Number of Peaks',
       'Number of Valleys', 'Pnn50', 'Pnn50pnn20', 'Quad_Cat', 'Ratio',
       'Rmssd', 'SD', 'SD_Blink_Duration', 'SD_Fixation_Duration',
       'SD_Microsac_Ampl', 'SD_Microsac_Dir', 'SD_Microsac_H_Amp',
       'SD_Microsac_Peak_Vel', 'SD_Microsac_V_Amp', 'SD_Saccade_Amplitude',
       'SD_Saccade_Direction', 'SD_Saccade_Duration', 'SD_Saccade_Length',
       'Sdnn', 'Sdsd', 'Skew_Blink_Duration', 'Skew_Fixation_Duration',
       'Skew_Microsac_Ampl', 'Skew_Microsac_Dir', 'Skew_Microsac_H_Amp',
       'Skew_Microsac_Peak_Vel', 'Skew_Microsac_V_Amp',
       'Skew_Saccade_Amplitude', 'Skew_Saccade_Direction',
       'Skew_Saccade_Duration', 'Skew_Saccade_Length', 'VLF', 'Variance',
       'pnn20']]

# we Subset the dataset for arousal classification
arousal_df = all_concatenated_df[['Bpm', 'First_Fixation_Duration', 'HF', 'Ibi', 'LF', 'Max', 'MaxRR',
       'Max_Blink_Duration', 'Max_Fixation_Duration', 'Max_Microsac_Ampl',
       'Max_Microsac_Dir', 'Max_Microsac_H_Amp', 'Max_Microsac_Peak_Vel',
       'Max_Microsac_V_Amp', 'Max_Saccade_Amplitude', 'Max_Saccade_Direction',
       'Max_Saccade_Duration', 'Max_Saccade_Length', 'Maximum', 'Mean',
       'MeanRR', 'Mean_Blink_Duration', 'Mean_Fixation_Duration',
       'Mean_Microsac_Ampl', 'Mean_Microsac_Dir', 'Mean_Microsac_H_Amp',
       'Mean_Microsac_Peak_Vel', 'Mean_Microsac_V_Amp',
       'Mean_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'Mean_Saccade_Duration', 'Mean_Saccade_Length', 'MedianRR', 'Min',
       'MinRR', 'Minimum', 'Num_of_Blink', 'Num_of_Fixations',
       'Num_of_Microsac', 'Num_of_Saccade', 'Number of Peaks',
       'Number of Valleys', 'Pnn50', 'Pnn50pnn20', 'Quad_Cat', 'Ratio',
       'Rmssd', 'SD', 'SD_Blink_Duration', 'SD_Fixation_Duration',
       'SD_Microsac_Ampl', 'SD_Microsac_Dir', 'SD_Microsac_H_Amp',
       'SD_Microsac_Peak_Vel', 'SD_Microsac_V_Amp', 'SD_Saccade_Amplitude',
       'SD_Saccade_Direction', 'SD_Saccade_Duration', 'SD_Saccade_Length',
       'Sdnn', 'Sdsd', 'Skew_Blink_Duration', 'Skew_Fixation_Duration',
       'Skew_Microsac_Ampl', 'Skew_Microsac_Dir', 'Skew_Microsac_H_Amp',
       'Skew_Microsac_Peak_Vel', 'Skew_Microsac_V_Amp',
       'Skew_Saccade_Amplitude', 'Skew_Saccade_Direction',
       'Skew_Saccade_Duration', 'Skew_Saccade_Length', 'VLF', 'Variance',
       'pnn20']]
# We Map the Quad_Cat values to respective arousal and valence labels
arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}

arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)

# Spliting the arousal dataset into feature matrix X_arousal and target vector y_arousal
X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']

# Spliting the valence dataset into feature matrix X_valence and target vector y_valence
X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']

# we Drop rows with missing values from X_arousal_train and y_arousal_train
X_arousal_train = X_arousal_train.dropna()
y_arousal_train = y_arousal_train.dropna()



# Spliting the arousal data into training and testing sets
X_arousal_train, X_arousal_test, y_arousal_train, y_arousal_test = train_test_split(X_arousal, y_arousal, test_size=0.1, random_state=42)

# Spliting the valence data into training and testing sets
X_valence_train, X_valence_test, y_valence_train, y_valence_test = train_test_split(X_valence, y_valence, test_size=0.1, random_state=42)





# a pipeline with preprocessing and KNN classifier
pipeline_arousal = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step: Center and scale
    ('knn', KNeighborsClassifier())  # KNN classifier
])

pipeline_valence = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step: Center and scale
    ('knn', KNeighborsClassifier())  # KNN classifier
])

# the hyperparameter grid for KNN
param_grid = {
    'knn__n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors
    'knn__weights': ['uniform', 'distance']  # Weighting scheme
}

# grid search with 10-fold cross-validation for arousal
grid_search_arousal = GridSearchCV(pipeline_arousal, param_grid, cv=10, scoring='accuracy', return_train_score=True)
grid_search_arousal.fit(X_arousal_train, y_arousal_train)

# grid search with 10-fold cross-validation for valence
grid_search_valence = GridSearchCV(pipeline_valence, param_grid, cv=10, scoring='accuracy', return_train_score=True)
grid_search_valence.fit(X_valence_train, y_valence_train)

# predictions on the arousal test set
y_arousal_pred = grid_search_arousal.predict(X_arousal_test)

# predictions on the valence test set
y_valence_pred = grid_search_valence.predict(X_valence_test)

# evaluation metrics for arousal
accuracy_arousal = accuracy_score(y_arousal_test, y_arousal_pred)
precision_arousal = precision_score(y_arousal_test, y_arousal_pred, average='weighted')
recall_arousal = recall_score(y_arousal_test, y_arousal_pred, average='weighted')
f1_arousal = f1_score(y_arousal_test, y_arousal_pred, average='weighted')

print("Arousal Metrics:")
print("Accuracy:", accuracy_arousal)
print("Precision:", precision_arousal)
print("Recall:", recall_arousal)
print("F1-score:", f1_arousal)
print()

# evaluation metrics for valence
accuracy_valence = accuracy_score(y_valence_test, y_valence_pred)
precision_valence = precision_score(y_valence_test, y_valence_pred, average='weighted')
recall_valence = recall_score(y_valence_test, y_valence_pred, average='weighted')
f1_valence = f1_score(y_valence_test, y_valence_pred, average='weighted')

print("Valence Metrics:")
print("Accuracy:", accuracy_valence)
print("Precision:", precision_valence)
print("Recall:", recall_valence)
print("F1-score:", f1_valence)

"""# KNN for all signlas and data"""

X = all_concatenated_df[['Bpm', 'First_Fixation_Duration', 'HF', 'Ibi', 'LF', 'Max', 'MaxRR',
       'Max_Blink_Duration', 'Max_Fixation_Duration', 'Max_Microsac_Ampl',
       'Max_Microsac_Dir', 'Max_Microsac_H_Amp', 'Max_Microsac_Peak_Vel',
       'Max_Microsac_V_Amp', 'Max_Saccade_Amplitude', 'Max_Saccade_Direction',
       'Max_Saccade_Duration', 'Max_Saccade_Length', 'Maximum', 'Mean',
       'MeanRR', 'Mean_Blink_Duration', 'Mean_Fixation_Duration',
       'Mean_Microsac_Ampl', 'Mean_Microsac_Dir', 'Mean_Microsac_H_Amp',
       'Mean_Microsac_Peak_Vel', 'Mean_Microsac_V_Amp',
       'Mean_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'Mean_Saccade_Duration', 'Mean_Saccade_Length', 'MedianRR', 'Min',
       'MinRR', 'Minimum', 'Num_of_Blink', 'Num_of_Fixations',
       'Num_of_Microsac', 'Num_of_Saccade', 'Number of Peaks',
       'Number of Valleys', 'Pnn50', 'Pnn50pnn20', 'Quad_Cat', 'Ratio',
       'Rmssd', 'SD', 'SD_Blink_Duration', 'SD_Fixation_Duration',
       'SD_Microsac_Ampl', 'SD_Microsac_Dir', 'SD_Microsac_H_Amp',
       'SD_Microsac_Peak_Vel', 'SD_Microsac_V_Amp', 'SD_Saccade_Amplitude',
       'SD_Saccade_Direction', 'SD_Saccade_Duration', 'SD_Saccade_Length',
       'Sdnn', 'Sdsd', 'Skew_Blink_Duration', 'Skew_Fixation_Duration',
       'Skew_Microsac_Ampl', 'Skew_Microsac_Dir', 'Skew_Microsac_H_Amp',
       'Skew_Microsac_Peak_Vel', 'Skew_Microsac_V_Amp',
       'Skew_Saccade_Amplitude', 'Skew_Saccade_Direction',
       'Skew_Saccade_Duration', 'Skew_Saccade_Length', 'VLF', 'Variance',
       'pnn20']]
y = all_concatenated_df['Quad_Cat']

# We Map the Quad_Cat values to respective labels
label_mapping = {0: 'High Arousal High Valence', 1: 'Low Arousal High Valence', 2: 'Low Arousal Low Valence', 3: 'High Arousal Low Valence'}

y = y.map(label_mapping)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# a pipeline with preprocessing and KNN classifier
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing step: Center and scale
    ('knn', KNeighborsClassifier())  # KNN classifier
])

# the hyperparameter grid for KNN
param_grid = {
    'knn__n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors
    'knn__weights': ['uniform', 'distance']  # Weighting scheme
}

# grid search with 10-fold cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy', return_train_score=True)
grid_search.fit(X_train, y_train)

# predictions on the test set
y_pred = grid_search.predict(X_test)

# evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Metrics:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)