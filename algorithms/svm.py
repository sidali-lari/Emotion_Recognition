# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H-kJIAOEYJTUI4l6mKmbqlsa3Zl05Dmw

# Importing the necessary libraries
"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from imblearn.over_sampling import SMOTE
from sklearn.utils import class_weight

"""# Importing the Dataset"""

df_gsr = pd.read_csv('GSR_FeaturesExtracted.csv')
df_ecg = pd.read_csv('ECG_FeaturesExtracted.csv')
df_eye_tracking = pd.read_csv('EyeTracking_FeaturesExtracted.csv')
#df_gsr_ecg = pd.read_csv('GSR+ECG.csv')
df_all_data = pd.read_csv('all_data.csv')

"""# Cleaning Eye tracking and all data from Nan vlaues"""

### -- Eye Trakcing
df_eye_tracking.dropna(inplace=True)
# Drop columns with NaN values
df_eye_tracking.dropna(axis=1, inplace=True)

### -- All Data
df_all_data.dropna(inplace=True)
# Drop columns with NaN values
df_all_data.dropna(axis=1, inplace=True)

"""# 4-Class GSR"""

# We extract the features and target variable
X = df_gsr.drop('Quad_Cat', axis=1)
y = df_gsr['Quad_Cat']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

#Split the data into training and testing sets - 90% for train and 10% for test
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=42)
smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)
sample_weights = class_weight.compute_sample_weight('balanced', y_train_bal)

# parameter grid for hyperparameter tuning
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'gamma': [0.001, 0.01, 0.1, 1, 10, 100],
    'kernel': ['rbf'],
    'class_weight': ['balanced', None]
}

#the SVM classifier
svm = SVC()
grid_search = GridSearchCV(svm, param_grid, cv=10, scoring='accuracy')
grid_search.fit(X_train_bal, y_train_bal, sample_weight=sample_weights)
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Predict
y_pred = grid_search.predict(X_test)

# evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

#evaluation metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

"""# GSR Arousal"""

# We Subset the dataset for arousal classification
arousal_df = df_gsr[['Quad_Cat', 'Mean', 'SD', 'Variance', 'Minimum', 'Maximum',
   'Number of Peaks', 'Number of Valleys', 'Ratio']].copy()
arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)

# Split the arousal dataset into feature matrix X_arousal and target vector y_arousal
X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']
scaler = StandardScaler()
X_arousal_scaled = scaler.fit_transform(X_arousal)
X_arousal_train, X_arousal_test, y_arousal_train, y_arousal_test = train_test_split(X_arousal_scaled, y_arousal, test_size=0.1, random_state=42)
smote = SMOTE(random_state=42)
X_arousal_train_bal, y_arousal_train_bal = smote.fit_resample(X_arousal_train, y_arousal_train)
sample_weights = class_weight.compute_sample_weight('balanced', y_arousal_train_bal)

# Parameter grid
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],  # Regularization parameter
    'gamma': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],  # Kernel coefficient for 'rbf' kernel
    'kernel': ['rbf'],  # Kernel type
    'class_weight': ['balanced', None]
}

#SVM classifier
svm = SVC()
grid_search = GridSearchCV(svm, param_grid, cv=10, scoring='accuracy')
grid_search.fit(X_arousal_train_bal, y_arousal_train_bal, sample_weight=sample_weights)
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Predict
y_arousal_pred = grid_search.predict(X_arousal_test)
# evaluation metrics for arousal
accuracy_arousal = accuracy_score(y_arousal_test, y_arousal_pred)
precision_arousal = precision_score(y_arousal_test, y_arousal_pred, average='weighted')
recall_arousal = recall_score(y_arousal_test, y_arousal_pred, average='weighted')
f1_arousal = f1_score(y_arousal_test, y_arousal_pred, average='weighted')

print("Arousal Metrics:")
print("Accuracy:", accuracy_arousal)
print("Precision:", precision_arousal)
print("Recall:", recall_arousal)
print("F1-score:", f1_arousal)
print()

"""# GSR Valence"""

valence_df = df_gsr[['Quad_Cat', 'Mean', 'SD', 'Variance', 'Minimum', 'Maximum',
       'Number of Peaks', 'Number of Valleys', 'Ratio']]
valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)


X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']
scaler = StandardScaler()
X_valence_scaled = scaler.fit_transform(X_valence)
X_valence_train, X_valence_test, y_valence_train, y_valence_test = train_test_split(X_valence_scaled, y_valence, test_size=0.1, random_state=42)
smote = SMOTE(random_state=42)
X_valence_train_bal, y_valence_train_bal = smote.fit_resample(X_valence_train, y_valence_train)

#parameter grid
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'gamma': [0.1, 1, 10, 100],
    'kernel': ['rbf']
}

# SVM classifier
svm = SVC()
grid_search = GridSearchCV(svm, param_grid, cv=10, scoring='accuracy')
grid_search.fit(X_valence_train_bal, y_valence_train_bal)
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)
# Predict on the test data using the best model
y_valence_pred = grid_search.predict(X_valence_test)

# Evaluation metrics for valence
accuracy_valence = accuracy_score(y_valence_test, y_valence_pred)
precision_valence = precision_score(y_valence_test, y_valence_pred, average='weighted')
recall_valence = recall_score(y_valence_test, y_valence_pred, average='weighted')
f1_valence = f1_score(y_valence_test, y_valence_pred, average='weighted')

print("Valence Metrics:")
print("Accuracy:", accuracy_valence)
print("Precision:", precision_valence)
print("Recall:", recall_valence)
print("F1-score:", f1_valence)
print()

"""# 4-Class ECG"""

X = df_ecg.drop('Quad_Cat', axis=1)
y = df_ecg['Quad_Cat']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# We Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=42)
smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)
sample_weights = class_weight.compute_sample_weight('balanced', y_train_bal)

# the parameter grid for hyperparameter tuning
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'gamma': [0.001, 0.01, 0.1, 1, 10, 100],
    'kernel': ['rbf'],
    'class_weight': ['balanced', None]
}

# SVM classifier
svm = SVC()
grid_search = GridSearchCV(svm, param_grid, cv=10, scoring='accuracy')
grid_search.fit(X_train_bal, y_train_bal, sample_weight=sample_weights)
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Predict on the test data using the best model
y_pred = grid_search.predict(X_test)

# evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

# evaluation metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

"""# ECG Arousal"""

df_ecg.columns

arousal_df = df_ecg[['Quad_Cat', 'Mean', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR',
       'MaxRR', 'LF', 'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd',
       'Pnn50', 'pnn20', 'Pnn50pnn20']].copy()
arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)

#Split the arousal dataset into feature matrix X_arousal and target vector y_arousal
X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']
scaler = StandardScaler()
X_arousal_scaled = scaler.fit_transform(X_arousal)
X_arousal_train, X_arousal_test, y_arousal_train, y_arousal_test = train_test_split(X_arousal_scaled, y_arousal, test_size=0.1, random_state=42)
smote = SMOTE(random_state=42)
X_arousal_train_bal, y_arousal_train_bal = smote.fit_resample(X_arousal_train, y_arousal_train)
sample_weights = class_weight.compute_sample_weight('balanced', y_arousal_train_bal)

# hyperparameter tuning
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],  # Regularization parameter
    'gamma': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],  # Kernel coefficient for 'rbf' kernel
    'kernel': ['rbf'],  # Kernel type
    'class_weight': ['balanced', None]
}

#SVM classifier
svm = SVC()
grid_search = GridSearchCV(svm, param_grid, cv=10, scoring='accuracy')
grid_search.fit(X_arousal_train_bal, y_arousal_train_bal, sample_weight=sample_weights)
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Predict on the test data using the best model
y_arousal_pred = grid_search.predict(X_arousal_test)
# evaluation metrics for arousal
accuracy_arousal = accuracy_score(y_arousal_test, y_arousal_pred)
precision_arousal = precision_score(y_arousal_test, y_arousal_pred, average='weighted')
recall_arousal = recall_score(y_arousal_test, y_arousal_pred, average='weighted')
f1_arousal = f1_score(y_arousal_test, y_arousal_pred, average='weighted')

print("Arousal Metrics:")
print("Accuracy:", accuracy_arousal)
print("Precision:", precision_arousal)
print("Recall:", recall_arousal)
print("F1-score:", f1_arousal)
print()

"""# ECG Valence"""

valence_df = df_ecg[['Quad_Cat', 'Mean', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR',
       'MaxRR', 'LF', 'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd',
       'Pnn50', 'pnn20', 'Pnn50pnn20']]
valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)

# Splitting the valence dataset into feature matrix X_valence and target vector y_valence
X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']
# Scaling the features
scaler = StandardScaler()
X_valence_scaled = scaler.fit_transform(X_valence)
X_valence_train, X_valence_test, y_valence_train, y_valence_test = train_test_split(X_valence_scaled, y_valence, test_size=0.1, random_state=42)
smote = SMOTE(random_state=42)
X_valence_train_bal, y_valence_train_bal = smote.fit_resample(X_valence_train, y_valence_train)

# parameter grid for hyperparameter tuning
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'gamma': [0.1, 1, 10, 100],
    'kernel': ['rbf']
}

# SVM classifier
svm = SVC()
grid_search = GridSearchCV(svm, param_grid, cv=10, scoring='accuracy')
grid_search.fit(X_valence_train_bal, y_valence_train_bal)
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)
# Predict on the test data
y_valence_pred = grid_search.predict(X_valence_test)

# Evaluation metrics for valence
accuracy_valence = accuracy_score(y_valence_test, y_valence_pred)
precision_valence = precision_score(y_valence_test, y_valence_pred, average='weighted')
recall_valence = recall_score(y_valence_test, y_valence_pred, average='weighted')
f1_valence = f1_score(y_valence_test, y_valence_pred, average='weighted')

print("Valence Metrics:")
print("Accuracy:", accuracy_valence)
print("Precision:", precision_valence)
print("Recall:", recall_valence)
print("F1-score:", f1_valence)
print()

"""# Eye Tracking 4-class"""

X = df_eye_tracking.drop('Quad_Cat', axis=1)
y = df_eye_tracking['Quad_Cat']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# We Split the data into training and testing sets - 90% for train and 10% for test
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=42)
smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)
sample_weights = class_weight.compute_sample_weight('balanced', y_train_bal)

# hyperparameter tuning
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'gamma': [0.001, 0.01, 0.1, 1, 10, 100],
    'kernel': ['rbf'],
    'class_weight': ['balanced', None]
}

#SVM classifier
svm = SVC()
grid_search = GridSearchCV(svm, param_grid, cv=10, scoring='accuracy')
grid_search.fit(X_train_bal, y_train_bal, sample_weight=sample_weights)
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Predict on the test data using the best model
y_pred = grid_search.predict(X_test)
# Calculating evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')
# Printing the evaluation metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

"""# SVM Eye Tracking Arousal"""

# We Subset the dataset for arousal classification
arousal_df = df_eye_tracking[['Quad_Cat', 'Num_of_Fixations', 'Mean_Fixation_Duration',
       'SD_Fixation_Duration', 'Skew_Fixation_Duration',
       'Max_Fixation_Duration', 'First_Fixation_Duration', 'Num_of_Saccade',
       'Mean_Saccade_Duration', 'SD_Saccade_Duration', 'Skew_Saccade_Duration',
       'Max_Saccade_Duration', 'Mean_Saccade_Amplitude',
       'SD_Saccade_Amplitude', 'Skew_Saccade_Amplitude',
       'Max_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'SD_Saccade_Direction', 'Skew_Saccade_Direction',
       'Max_Saccade_Direction', 'Mean_Saccade_Length', 'SD_Saccade_Length',
       'Skew_Saccade_Length', 'Max_Saccade_Length', 'Num_of_Blink',
       'Mean_Blink_Duration', 'SD_Blink_Duration', 'Skew_Blink_Duration',
       'Max_Blink_Duration', 'Num_of_Microsac', 'Mean_Microsac_Peak_Vel',
       'SD_Microsac_Peak_Vel', 'Skew_Microsac_Peak_Vel',
       'Max_Microsac_Peak_Vel', 'Mean_Microsac_Ampl', 'SD_Microsac_Ampl',
       'Skew_Microsac_Ampl', 'Max_Microsac_Ampl', 'Mean_Microsac_Dir',
       'SD_Microsac_Dir', 'Skew_Microsac_Dir', 'Max_Microsac_Dir',
       'Mean_Microsac_H_Amp', 'SD_Microsac_H_Amp', 'Skew_Microsac_H_Amp',
       'Max_Microsac_H_Amp', 'Mean_Microsac_V_Amp', 'SD_Microsac_V_Amp',
       'Skew_Microsac_V_Amp', 'Max_Microsac_V_Amp']].copy()
arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)

# We Split the arousal dataset into feature matrix X_arousal and target vector y_arousal
X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']

# Scaling the features
scaler = StandardScaler()
X_arousal_scaled = scaler.fit_transform(X_arousal)
X_arousal_train, X_arousal_test, y_arousal_train, y_arousal_test = train_test_split(X_arousal_scaled, y_arousal, test_size=0.1, random_state=42)
smote = SMOTE(random_state=42)
X_arousal_train_bal, y_arousal_train_bal = smote.fit_resample(X_arousal_train, y_arousal_train)
sample_weights = class_weight.compute_sample_weight('balanced', y_arousal_train_bal)

# Parameter grid for hyperparameter tuning
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],  # Regularization parameter
    'gamma': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],  # Kernel coefficient for 'rbf' kernel
    'kernel': ['rbf'],  # Kernel type
    'class_weight': ['balanced', None]
}

#SVM classifier
svm = SVC()
grid_search = GridSearchCV(svm, param_grid, cv=10, scoring='accuracy')
grid_search.fit(X_arousal_train_bal, y_arousal_train_bal, sample_weight=sample_weights)
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Predict on the test data using the best model
y_arousal_pred = grid_search.predict(X_arousal_test)
# evaluation metrics for arousal
accuracy_arousal = accuracy_score(y_arousal_test, y_arousal_pred)
precision_arousal = precision_score(y_arousal_test, y_arousal_pred, average='weighted')
recall_arousal = recall_score(y_arousal_test, y_arousal_pred, average='weighted')
f1_arousal = f1_score(y_arousal_test, y_arousal_pred, average='weighted')

print("Arousal Metrics:")
print("Accuracy:", accuracy_arousal)
print("Precision:", precision_arousal)
print("Recall:", recall_arousal)
print("F1")

"""# SVM Eye Tracking - Valence"""

valence_df = df_eye_tracking[['Quad_Cat', 'Num_of_Fixations', 'Mean_Fixation_Duration',
       'SD_Fixation_Duration', 'Skew_Fixation_Duration',
       'Max_Fixation_Duration', 'First_Fixation_Duration', 'Num_of_Saccade',
       'Mean_Saccade_Duration', 'SD_Saccade_Duration', 'Skew_Saccade_Duration',
       'Max_Saccade_Duration', 'Mean_Saccade_Amplitude',
       'SD_Saccade_Amplitude', 'Skew_Saccade_Amplitude',
       'Max_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'SD_Saccade_Direction', 'Skew_Saccade_Direction',
       'Max_Saccade_Direction', 'Mean_Saccade_Length', 'SD_Saccade_Length',
       'Skew_Saccade_Length', 'Max_Saccade_Length', 'Num_of_Blink',
       'Mean_Blink_Duration', 'SD_Blink_Duration', 'Skew_Blink_Duration',
       'Max_Blink_Duration', 'Num_of_Microsac', 'Mean_Microsac_Peak_Vel',
       'SD_Microsac_Peak_Vel', 'Skew_Microsac_Peak_Vel',
       'Max_Microsac_Peak_Vel', 'Mean_Microsac_Ampl', 'SD_Microsac_Ampl',
       'Skew_Microsac_Ampl', 'Max_Microsac_Ampl', 'Mean_Microsac_Dir',
       'SD_Microsac_Dir', 'Skew_Microsac_Dir', 'Max_Microsac_Dir',
       'Mean_Microsac_H_Amp', 'SD_Microsac_H_Amp', 'Skew_Microsac_H_Amp',
       'Max_Microsac_H_Amp', 'Mean_Microsac_V_Amp', 'SD_Microsac_V_Amp',
       'Skew_Microsac_V_Amp', 'Max_Microsac_V_Amp']]
valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)

# Splitting the valence dataset into feature matrix X_valence and target vector y_valence
X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']
scaler = StandardScaler()
X_valence_scaled = scaler.fit_transform(X_valence)
X_valence_train, X_valence_test, y_valence_train, y_valence_test = train_test_split(X_valence_scaled, y_valence, test_size=0.1, random_state=42)
smote = SMOTE(random_state=42)
X_valence_train_bal, y_valence_train_bal = smote.fit_resample(X_valence_train, y_valence_train)

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'gamma': [0.1, 1, 10, 100],
    'kernel': ['rbf']
}

# SVM classifier
svm = SVC()
grid_search = GridSearchCV(svm, param_grid, cv=10, scoring='accuracy')
grid_search.fit(X_valence_train_bal, y_valence_train_bal)
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)
# Predict on the test data using the best model
y_valence_pred = grid_search.predict(X_valence_test)

# Evaluation metrics for valence
accuracy_valence = accuracy_score(y_valence_test, y_valence_pred)
precision_valence = precision_score(y_valence_test, y_valence_pred, average='weighted')
recall_valence = recall_score(y_valence_test, y_valence_pred, average='weighted')
f1_valence = f1_score(y_valence_test, y_valence_pred, average='weighted')

print("Valence Metrics:")
print("Accuracy:", accuracy_valence)
print("Precision:", precision_valence)
print("Recall:", recall_valence)
print("F1-score:", f1_valence)
print()

"""# All Data 4-class"""

X = df_all_data.drop('Quad_Cat', axis=1)
y = df_all_data['Quad_Cat']

# Scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# We Split the data into training and testing sets - 90% for train and 10% for test
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=42)
smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)
sample_weights = class_weight.compute_sample_weight('balanced', y_train_bal)

# the parameter grid for hyperparameter tuning
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'gamma': [0.001, 0.01, 0.1, 1, 10, 100],
    'kernel': ['rbf'],
    'class_weight': ['balanced', None]
}

# the SVM classifier
svm = SVC()
grid_search = GridSearchCV(svm, param_grid, cv=10, scoring='accuracy')
grid_search.fit(X_train_bal, y_train_bal, sample_weight=sample_weights)

# the best hyperparameters found by grid search
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)
y_pred = grid_search.predict(X_test)
# evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')
# evaluation metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

"""# SVM All Data - Arousal"""

# Subset the dataset for arousal classification
arousal_df = df_all_data[['Quad_Cat', 'Num_of_Fixations', 'Mean_Fixation_Duration',
       'SD_Fixation_Duration', 'Skew_Fixation_Duration',
       'Max_Fixation_Duration', 'First_Fixation_Duration', 'Num_of_Saccade',
       'Mean_Saccade_Duration', 'SD_Saccade_Duration', 'Skew_Saccade_Duration',
       'Max_Saccade_Duration', 'Mean_Saccade_Amplitude',
       'SD_Saccade_Amplitude', 'Skew_Saccade_Amplitude',
       'Max_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'SD_Saccade_Direction', 'Skew_Saccade_Direction',
       'Max_Saccade_Direction', 'Mean_Saccade_Length', 'SD_Saccade_Length',
       'Skew_Saccade_Length', 'Max_Saccade_Length', 'Num_of_Blink',
       'Mean_Blink_Duration', 'SD_Blink_Duration', 'Skew_Blink_Duration',
       'Max_Blink_Duration', 'Num_of_Microsac', 'Mean_Microsac_Peak_Vel',
       'SD_Microsac_Peak_Vel', 'Skew_Microsac_Peak_Vel',
       'Max_Microsac_Peak_Vel', 'Mean_Microsac_Ampl', 'SD_Microsac_Ampl',
       'Skew_Microsac_Ampl', 'Max_Microsac_Ampl', 'Mean_Microsac_Dir',
       'SD_Microsac_Dir', 'Skew_Microsac_Dir', 'Max_Microsac_Dir',
       'Mean_Microsac_H_Amp', 'SD_Microsac_H_Amp', 'Skew_Microsac_H_Amp',
       'Max_Microsac_H_Amp', 'Mean_Microsac_V_Amp', 'SD_Microsac_V_Amp',
       'Skew_Microsac_V_Amp', 'Max_Microsac_V_Amp', 'Mean', 'SD', 'Variance',
       'Minimum', 'Maximum', 'Number of Peaks', 'Number of Valleys', 'Ratio',
       'Mean.1', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR', 'MaxRR', 'LF',
       'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd', 'Pnn50', 'pnn20',
       'Pnn50pnn20']].copy()
arousal_mapping = {0: 'High Arousal', 1: 'Low Arousal', 2: 'Low Arousal', 3: 'High Arousal'}
arousal_df['Quad_Cat'] = arousal_df['Quad_Cat'].map(arousal_mapping)

# Split the arousal dataset into feature matrix X_arousal and target vector y_arousal
X_arousal = arousal_df.drop('Quad_Cat', axis=1)
y_arousal = arousal_df['Quad_Cat']
scaler = StandardScaler()
X_arousal_scaled = scaler.fit_transform(X_arousal)
X_arousal_train, X_arousal_test, y_arousal_train, y_arousal_test = train_test_split(X_arousal_scaled, y_arousal, test_size=0.1, random_state=42)
smote = SMOTE(random_state=42)
X_arousal_train_bal, y_arousal_train_bal = smote.fit_resample(X_arousal_train, y_arousal_train)
sample_weights = class_weight.compute_sample_weight('balanced', y_arousal_train_bal)

# Parameter grid for hyperparameter tuning
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],  # Regularization parameter
    'gamma': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],  # Kernel coefficient for 'rbf' kernel
    'kernel': ['rbf'],  # Kernel type
    'class_weight': ['balanced', None]
}

#SVM classifier
svm = SVC()
grid_search = GridSearchCV(svm, param_grid, cv=10, scoring='accuracy')
grid_search.fit(X_arousal_train_bal, y_arousal_train_bal, sample_weight=sample_weights)
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)
y_arousal_pred = grid_search.predict(X_arousal_test)
# evaluation metrics for arousal
accuracy_arousal = accuracy_score(y_arousal_test, y_arousal_pred)
precision_arousal = precision_score(y_arousal_test, y_arousal_pred, average='weighted')
recall_arousal = recall_score(y_arousal_test, y_arousal_pred, average='weighted')
f1_arousal = f1_score(y_arousal_test, y_arousal_pred, average='weighted')

print("Arousal Metrics:")
print("Accuracy:", accuracy_arousal)
print("Precision:", precision_arousal)
print("Recall:", recall_arousal)
print("F1")

"""# SVM All Data Valence"""

valence_df = df_all_data[['Quad_Cat', 'Num_of_Fixations', 'Mean_Fixation_Duration',
       'SD_Fixation_Duration', 'Skew_Fixation_Duration',
       'Max_Fixation_Duration', 'First_Fixation_Duration', 'Num_of_Saccade',
       'Mean_Saccade_Duration', 'SD_Saccade_Duration', 'Skew_Saccade_Duration',
       'Max_Saccade_Duration', 'Mean_Saccade_Amplitude',
       'SD_Saccade_Amplitude', 'Skew_Saccade_Amplitude',
       'Max_Saccade_Amplitude', 'Mean_Saccade_Direction',
       'SD_Saccade_Direction', 'Skew_Saccade_Direction',
       'Max_Saccade_Direction', 'Mean_Saccade_Length', 'SD_Saccade_Length',
       'Skew_Saccade_Length', 'Max_Saccade_Length', 'Num_of_Blink',
       'Mean_Blink_Duration', 'SD_Blink_Duration', 'Skew_Blink_Duration',
       'Max_Blink_Duration', 'Num_of_Microsac', 'Mean_Microsac_Peak_Vel',
       'SD_Microsac_Peak_Vel', 'Skew_Microsac_Peak_Vel',
       'Max_Microsac_Peak_Vel', 'Mean_Microsac_Ampl', 'SD_Microsac_Ampl',
       'Skew_Microsac_Ampl', 'Max_Microsac_Ampl', 'Mean_Microsac_Dir',
       'SD_Microsac_Dir', 'Skew_Microsac_Dir', 'Max_Microsac_Dir',
       'Mean_Microsac_H_Amp', 'SD_Microsac_H_Amp', 'Skew_Microsac_H_Amp',
       'Max_Microsac_H_Amp', 'Mean_Microsac_V_Amp', 'SD_Microsac_V_Amp',
       'Skew_Microsac_V_Amp', 'Max_Microsac_V_Amp', 'Mean', 'SD', 'Variance',
       'Minimum', 'Maximum', 'Number of Peaks', 'Number of Valleys', 'Ratio',
       'Mean.1', 'Min', 'Max', 'MeanRR', 'MedianRR', 'MinRR', 'MaxRR', 'LF',
       'HF', 'VLF', 'Ibi', 'Bpm', 'Sdnn', 'Sdsd', 'Rmssd', 'Pnn50', 'pnn20',
       'Pnn50pnn20']]
valence_mapping = {0: 'High Valence', 1: 'High Valence', 2: 'Low Valence', 3: 'Low Valence'}
valence_df['Quad_Cat'] = valence_df['Quad_Cat'].map(valence_mapping)
X_valence = valence_df.drop('Quad_Cat', axis=1)
y_valence = valence_df['Quad_Cat']
scaler = StandardScaler()
X_valence_scaled = scaler.fit_transform(X_valence)
X_valence_train, X_valence_test, y_valence_train, y_valence_test = train_test_split(X_valence_scaled, y_valence, test_size=0.1, random_state=42)
smote = SMOTE(random_state=42)
X_valence_train_bal, y_valence_train_bal = smote.fit_resample(X_valence_train, y_valence_train)
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'gamma': [0.1, 1, 10, 100],
    'kernel': ['rbf']
}
svm = SVC()
grid_search = GridSearchCV(svm, param_grid, cv=10, scoring='accuracy')
grid_search.fit(X_valence_train_bal, y_valence_train_bal)
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)
y_valence_pred = grid_search.predict(X_valence_test)
accuracy_valence = accuracy_score(y_valence_test, y_valence_pred)
precision_valence = precision_score(y_valence_test, y_valence_pred, average='weighted')
recall_valence = recall_score(y_valence_test, y_valence_pred, average='weighted')
f1_valence = f1_score(y_valence_test, y_valence_pred, average='weighted')

print("Valence Metrics:")
print("Accuracy:", accuracy_valence)
print("Precision:", precision_valence)
print("Recall:", recall_valence)
print("F1-score:", f1_valence)
print()

!git init

!git config --global user.email "larianesidali@gmail.com"
!git config --global user.name "sidali-lari"

!git remote add origin https://github.com/sidali-lari/Emotion_Recognition.git

!git add .

!git commit -m "svm architecture"

!git branch

!git push https://sidali-lari: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@github.com/sidali-lari/Emotion_Recognition.git master

!git status

!ls